{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Ultralytics YOLO\n",
    "\n",
    "In this notebook we put Ultralytics YOLO11 into practice. We will detect objects in several images, explore confidence and IoU thresholds, and build intuition for Intersection over Union (IoU) and Non-Max Suppression (NMS) without going too deep into theory.\n",
    "\n",
    "By the end of this notebook, you'll understand how to:\n",
    "- Run detection on multiple images\n",
    "- Tune confidence and IoU thresholds\n",
    "- Understand what IoU measures and why it matters\n",
    "- Grasp the concept of Non-Max Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Model Loading](#Setup-and-Model-Loading)\n",
    "2. [Quick Recap: What is Object Detection?](#Quick-Recap-What-is-Object-Detection)\n",
    "3. [Detection on Multiple Images](#Detection-on-Multiple-Images)\n",
    "4. [Confidence and IoU Thresholds](#Confidence-and-IoU-Thresholds)\n",
    "5. [Intersection over Union (IoU)](#Intersection-over-Union-IoU)\n",
    "6. [Non-Max Suppression (NMS)](#Non-Max-Suppression-NMS)\n",
    "7. [Optional: Webcam Detection](#Optional-Webcam-Detection)\n",
    "8. [Recap and Exercises](#Recap-and-Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Model Loading\n",
    "\n",
    "Let's import the necessary libraries and load our YOLO11 detection model.\n",
    "\n",
    "> **Note:** If the images in `../images/` are missing, run `07_intro_to_ultralytics.ipynb` first to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install Ultralytics and OpenCV in fresh environments (e.g. Colab)\n",
    "# %pip install ultralytics opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load YOLO11 nano model for detection\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "print(f\"Model loaded: {model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Recap: What is Object Detection?\n",
    "\n",
    "Object detection goes beyond image classification. Instead of just saying \"this image contains a dog,\" detection tells you:\n",
    "\n",
    "- **Where** each object is (bounding box coordinates)\n",
    "- **What** each object is (class label)\n",
    "- **How confident** the model is (confidence score)\n",
    "\n",
    "### What the Detector Outputs\n",
    "\n",
    "For each detected object:\n",
    "- **Bounding box**: [x1, y1, x2, y2] coordinates defining the rectangle\n",
    "- **Class label**: The type of object (e.g., \"person\", \"car\", \"dog\")\n",
    "- **Confidence score**: Probability that the detection is correct (0 to 1)\n",
    "\n",
    "### COCO Dataset\n",
    "\n",
    "Our model is pre-trained on the [COCO dataset](https://cocodataset.org/), which contains 80 common object categories including people, vehicles, animals, and everyday objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection on Multiple Images\n",
    "\n",
    "Let's run detection on several images to see YOLO in action. We'll use the images downloaded in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of local image paths\n",
    "image_paths = [\n",
    "    \"../images/yolo_dog_cat.jpg\",\n",
    "    \"../images/yolo_beach_scene.jpg\",\n",
    "    \"../images/yolo_traffic.jpg\",\n",
    "    \"../images/yolo_phones_on_table.jpg\",\n",
    "    \"../images/yolo_einstein_head.jpg\",\n",
    "]\n",
    "\n",
    "# Run detection on each image\n",
    "for path in image_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: {path} not found. Run 07_intro_to_ultralytics.ipynb first.\")\n",
    "        continue\n",
    "    \n",
    "    img_bgr = cv2.imread(path)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    results = model(img_rgb)\n",
    "    annotated = results[0].plot()\n",
    "    \n",
    "    # Count detections\n",
    "    num_detections = len(results[0].boxes)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(annotated)\n",
    "    plt.title(f\"{os.path.basename(path)} - {num_detections} objects detected\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- **Dogs and cats**: Pets detected with high confidence\n",
    "- **Beach scene**: People, sports equipment, personal items\n",
    "- **Traffic**: Vehicles, pedestrians, traffic infrastructure\n",
    "- **Phones**: Electronic devices, sometimes mistaken for similar objects\n",
    "- **Einstein portrait**: Person detection (historical photos work too!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence and IoU Thresholds\n",
    "\n",
    "When running inference, two key parameters control the output:\n",
    "\n",
    "- **`conf`** (confidence threshold): Minimum confidence score for a detection to be kept\n",
    "- **`iou`** (IoU threshold for NMS): Controls how overlapping boxes are filtered\n",
    "\n",
    "Let's see how these affect detection results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Confidence Threshold\n",
    "\n",
    "The confidence threshold filters out weak detections. Let's see its effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test image\n",
    "img_bgr = cv2.imread(\"../images/yolo_traffic.jpg\")\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Test different confidence thresholds\n",
    "conf_values = [0.1, 0.25, 0.5, 0.75]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, conf in zip(axes, conf_values):\n",
    "    results = model(img_rgb, conf=conf)\n",
    "    num_det = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "    \n",
    "    ax.imshow(results[0].plot())\n",
    "    ax.set_title(f\"conf={conf}, detections={num_det}\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Effect of Confidence Threshold\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insights:**\n",
    "\n",
    "- **Lower `conf` (e.g., 0.1)**: More detections, including uncertain ones (more false positives)\n",
    "- **Higher `conf` (e.g., 0.75)**: Fewer detections, only high-confidence ones (more false negatives)\n",
    "- **Default `conf=0.25`**: Good balance for most applications\n",
    "\n",
    "> **Performance note:** For this course, `yolo11n.pt` (nano) is usually sufficient on CPU. Larger models like `yolo11m.pt`, `yolo11l.pt`, or `yolo11x.pt` provide higher accuracy but are slower on CPU and benefit significantly from GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying IoU Threshold\n",
    "\n",
    "The IoU threshold controls Non-Max Suppression (NMS), which removes duplicate detections of the same object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different IoU thresholds for NMS\n",
    "iou_values = [0.3, 0.5, 0.7]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, iou in zip(axes, iou_values):\n",
    "    results = model(img_rgb, conf=0.25, iou=iou)\n",
    "    num_det = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "    \n",
    "    ax.imshow(results[0].plot())\n",
    "    ax.set_title(f\"iou={iou}, detections={num_det}\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Effect of IoU Threshold (NMS)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insights:**\n",
    "\n",
    "- **Lower `iou` (e.g., 0.3)**: Stricter NMS, removes more overlapping boxes\n",
    "- **Higher `iou` (e.g., 0.7)**: Relaxed NMS, keeps more overlapping boxes\n",
    "- **Default `iou=0.7`**: Allows nearby but distinct objects to be detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection over Union (IoU)\n",
    "\n",
    "IoU (also called Jaccard Index) is a fundamental metric in object detection. It measures how much two bounding boxes overlap.\n",
    "\n",
    "### The Formula\n",
    "\n",
    "$$\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}$$\n",
    "\n",
    "- **IoU = 1.0**: Perfect overlap (identical boxes)\n",
    "- **IoU = 0.0**: No overlap at all\n",
    "- **IoU ≈ 0.5**: Moderate overlap\n",
    "\n",
    "Let's implement and visualize IoU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union between two boxes.\n",
    "    Box format: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    # Calculate intersection coordinates\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    inter_w = max(0, x2 - x1)\n",
    "    inter_h = max(0, y2 - y1)\n",
    "    inter_area = inter_w * inter_h\n",
    "    \n",
    "    # Calculate union area\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = area1 + area2 - inter_area\n",
    "    \n",
    "    # Return IoU\n",
    "    return inter_area / union_area if union_area > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_iou(box1, box2, img_size=200):\n",
    "    \"\"\"\n",
    "    Visualize two boxes and their IoU on a blank image.\n",
    "    \"\"\"\n",
    "    # Create blank image\n",
    "    img = np.ones((img_size, img_size, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Draw boxes (Red for box1, Blue for box2)\n",
    "    cv2.rectangle(img, (int(box1[0]), int(box1[1])), (int(box1[2]), int(box1[3])), (255, 0, 0), 2)\n",
    "    cv2.rectangle(img, (int(box2[0]), int(box2[1])), (int(box2[2]), int(box2[3])), (0, 0, 255), 2)\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = compute_iou(box1, box2)\n",
    "    \n",
    "    return img, iou\n",
    "\n",
    "# Example 1: High overlap\n",
    "box1_high = [30, 30, 120, 120]\n",
    "box2_high = [50, 50, 140, 140]\n",
    "\n",
    "# Example 2: Low overlap\n",
    "box1_low = [20, 20, 80, 80]\n",
    "box2_low = [100, 100, 180, 180]\n",
    "\n",
    "# Example 3: Moderate overlap\n",
    "box1_med = [30, 50, 110, 130]\n",
    "box2_med = [80, 60, 160, 140]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for ax, (b1, b2), title in zip(axes, \n",
    "                                [(box1_high, box2_high), (box1_med, box2_med), (box1_low, box2_low)],\n",
    "                                [\"High Overlap\", \"Moderate Overlap\", \"Low Overlap\"]):\n",
    "    img, iou = visualize_iou(b1, b2)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{title}\\nIoU = {iou:.3f}\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"IoU Examples (Red and Blue boxes)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why IoU Matters\n",
    "\n",
    "IoU is used in two critical ways:\n",
    "\n",
    "1. **Evaluation**: Compare predicted boxes to ground truth boxes. An IoU > 0.5 is typically considered a correct detection.\n",
    "\n",
    "2. **Non-Max Suppression**: When multiple boxes detect the same object, IoU determines which duplicates to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Max Suppression (NMS)\n",
    "\n",
    "Object detection models often produce multiple overlapping boxes for the same object. Non-Max Suppression (NMS) removes these duplicates.\n",
    "\n",
    "### How NMS Works (Conceptual)\n",
    "\n",
    "1. **Sort** all detections by confidence score (highest first)\n",
    "2. **Select** the box with highest confidence\n",
    "3. **Remove** all other boxes that have IoU > threshold with the selected box\n",
    "4. **Repeat** until no boxes remain\n",
    "\n",
    "### Example (Toy Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multiple overlapping detections for the same object\n",
    "print(\"Simulated overlapping detections:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "fake_boxes = [\n",
    "    {\"id\": 1, \"box\": [100, 100, 200, 200], \"conf\": 0.95},  # Highest confidence\n",
    "    {\"id\": 2, \"box\": [105, 98, 205, 198], \"conf\": 0.82},   # Overlaps with box 1\n",
    "    {\"id\": 3, \"box\": [110, 102, 210, 202], \"conf\": 0.78},  # Overlaps with box 1\n",
    "    {\"id\": 4, \"box\": [300, 100, 400, 200], \"conf\": 0.90},  # Different object\n",
    "]\n",
    "\n",
    "for det in fake_boxes:\n",
    "    print(f\"Box {det['id']}: conf={det['conf']:.2f}, coords={det['box']}\")\n",
    "\n",
    "# Visualize the overlapping boxes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Before NMS (all boxes)\n",
    "ax = axes[0]\n",
    "img_before = np.ones((300, 500, 3), dtype=np.uint8) * 240  # Light gray background\n",
    "\n",
    "colors = [(255, 0, 0), (0, 200, 0), (0, 0, 255), (255, 165, 0)]  # Red, Green, Blue, Orange\n",
    "for i, det in enumerate(fake_boxes):\n",
    "    box = det[\"box\"]\n",
    "    color = colors[i]\n",
    "    cv2.rectangle(img_before, (box[0], box[1]), (box[2], box[3]), color, 3)\n",
    "    # Add label with confidence\n",
    "    label = f\"Box {det['id']} ({det['conf']:.2f})\"\n",
    "    cv2.putText(img_before, label, (box[0], box[1] - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "ax.imshow(img_before)\n",
    "ax.set_title(\"BEFORE NMS: 4 overlapping detections\", fontsize=12)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Right plot: After NMS (only kept boxes)\n",
    "ax = axes[1]\n",
    "img_after = np.ones((300, 500, 3), dtype=np.uint8) * 240\n",
    "\n",
    "# Box 1 (highest conf, kept) and Box 4 (different object, kept)\n",
    "kept_boxes = [fake_boxes[0], fake_boxes[3]]  # Box 1 and Box 4\n",
    "kept_colors = [colors[0], colors[3]]\n",
    "\n",
    "for det, color in zip(kept_boxes, kept_colors):\n",
    "    box = det[\"box\"]\n",
    "    cv2.rectangle(img_after, (box[0], box[1]), (box[2], box[3]), color, 3)\n",
    "    label = f\"Box {det['id']} ({det['conf']:.2f})\"\n",
    "    cv2.putText(img_after, label, (box[0], box[1] - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "ax.imshow(img_after)\n",
    "ax.set_title(\"AFTER NMS: 2 boxes kept (duplicates removed)\", fontsize=12)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Non-Max Suppression Example\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nApplying NMS with IoU threshold = 0.5:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate IoUs between box 1 and others\n",
    "for i in range(1, len(fake_boxes)):\n",
    "    iou = compute_iou(fake_boxes[0][\"box\"], fake_boxes[i][\"box\"])\n",
    "    status = \"REMOVE\" if iou > 0.5 else \"KEEP\"\n",
    "    print(f\"Box 1 vs Box {fake_boxes[i]['id']}: IoU={iou:.3f} → {status}\")\n",
    "\n",
    "print(\"\\nResult: Keep Box 1 (highest conf) and Box 4 (different object)\")\n",
    "print(\"Remove Boxes 2 and 3 (duplicates of Box 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS in Ultralytics YOLO\n",
    "\n",
    "Ultralytics applies NMS **internally** during inference. You control it with:\n",
    "\n",
    "```python\n",
    "results = model(image, conf=0.25, iou=0.7)\n",
    "```\n",
    "\n",
    "- `conf=0.25`: Discard detections with confidence < 0.25 **before** NMS\n",
    "- `iou=0.7`: Remove overlapping boxes with IoU > 0.7 **during** NMS\n",
    "\n",
    "Lower IoU threshold → more aggressive removal of overlapping boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Webcam Detection\n",
    "\n",
    "If you have a webcam, you can try real-time detection. This may not work in all environments (hosted notebooks, remote servers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Real-time webcam detection\n",
    "# Uncomment the code below to try it\n",
    "\n",
    "# import cv2\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# model = YOLO(\"yolo11n.pt\")\n",
    "# cap = cv2.VideoCapture(0)  # Default webcam\n",
    "\n",
    "# print(\"Press 'q' to quit\")\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     \n",
    "#     # Run detection\n",
    "#     results = model(frame, verbose=False)\n",
    "#     annotated = results[0].plot()\n",
    "#     \n",
    "#     # Display\n",
    "#     cv2.imshow(\"YOLO11 Webcam Detection\", annotated)\n",
    "#     \n",
    "#     # Exit on 'q'\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Webcam code is commented out. Uncomment to try real-time detection.\")\n",
    "print(\"Note: This requires a local environment with webcam access.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap and Exercises\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Object detection** locates objects with bounding boxes, class labels, and confidence scores\n",
    "- **Confidence threshold** (`conf`): Filters out weak detections before NMS\n",
    "- **IoU threshold** (`iou`): Controls how aggressively NMS removes overlapping boxes\n",
    "- **IoU formula**: Intersection area / Union area (measures box overlap)\n",
    "- **NMS**: Removes duplicate detections by keeping only the highest-confidence box for overlapping detections\n",
    "- **Trade-offs**: Lower `conf` → more detections (more false positives); Lower `iou` → stricter duplicate removal\n",
    "- **Pixel-level segmentation and human pose estimation**: See `07b_segmentation_and_pose_with_ultralytics.ipynb` for masks and keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Count Detections per Class\n",
    "\n",
    "Create a function that counts how many objects of each class were detected in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Count detections per class\n",
    "# Best practice: Load the model once and reuse it for multiple images\n",
    "\n",
    "def count_detections_per_class(image_path, model):\n",
    "    \"\"\"\n",
    "    Count how many objects of each class are detected.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        model: Pre-loaded YOLO model instance (reuse for efficiency)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary: {class_name: count}\n",
    "    \"\"\"\n",
    "    # TODO: Load image and run detection\n",
    "    # img = cv2.imread(image_path)\n",
    "    # img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # results = model(img_rgb)\n",
    "    # r = results[0]\n",
    "    \n",
    "    # TODO: Count objects per class\n",
    "    # class_counts = {}\n",
    "    # for box in r.boxes:\n",
    "    #     cls_id = int(box.cls)\n",
    "    #     class_name = r.names[cls_id]\n",
    "    #     if class_name not in class_counts:\n",
    "    #         class_counts[class_name] = 0\n",
    "    #     class_counts[class_name] += 1\n",
    "    \n",
    "    # TODO: Print results\n",
    "    # print(f\"Detections in {os.path.basename(image_path)}:\")\n",
    "    # for class_name, count in sorted(class_counts.items()):\n",
    "    #     print(f\"  {class_name}: {count}\")\n",
    "    \n",
    "    # return class_counts\n",
    "    pass  # Remove this when you complete the TODO\n",
    "\n",
    "# Test the function - load model once, reuse for multiple calls\n",
    "# model = YOLO(\"yolo11n.pt\")\n",
    "# count_detections_per_class(\"../images/yolo_traffic.jpg\", model)\n",
    "# count_detections_per_class(\"../images/yolo_beach_scene.jpg\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Model Variants\n",
    "\n",
    "Compare `yolo11n.pt`, `yolo11s.pt`, and `yolo11m.pt` on the same image. Measure inference time and count detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compare model variants\n",
    "import time\n",
    "\n",
    "def compare_models(image_path, model_names=[\"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\"]):\n",
    "    \"\"\"\n",
    "    Compare different YOLO11 models on the same image.\n",
    "    \"\"\"\n",
    "    # TODO: Load image\n",
    "    # img = cv2.imread(image_path)\n",
    "    # img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    print(f\"Comparing models on {os.path.basename(image_path)}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<15} | {'Detections':<12} | {'Inference Time (ms)':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # TODO: Load model\n",
    "        # model = YOLO(model_name)\n",
    "        \n",
    "        # TODO: Measure inference time\n",
    "        # start = time.time()\n",
    "        # results = model(img_rgb)\n",
    "        # elapsed = (time.time() - start) * 1000\n",
    "        \n",
    "        # TODO: Count detections\n",
    "        # num_det = len(results[0].boxes)\n",
    "        \n",
    "        # TODO: Print results\n",
    "        # print(f\"{model_name:<15} | {num_det:<12} | {elapsed:.1f}\")\n",
    "        pass  # Remove this when you complete the TODO\n",
    "\n",
    "# Test the function\n",
    "# compare_models(\"../images/yolo_traffic.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Count People Across Images\n",
    "\n",
    "Count the total number of \"person\" detections across multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Count people across multiple images\n",
    "def count_people_in_images(image_paths):\n",
    "    \"\"\"\n",
    "    Count total number of 'person' detections across all images.\n",
    "    \"\"\"\n",
    "    # TODO: Initialize model and counter\n",
    "    # model = YOLO(\"yolo11n.pt\")\n",
    "    # total_people = 0\n",
    "    \n",
    "    for path in image_paths:\n",
    "        # TODO: Load and detect\n",
    "        # img = cv2.imread(path)\n",
    "        # img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # results = model(img_rgb)\n",
    "        # r = results[0]\n",
    "        \n",
    "        # TODO: Count people in this image\n",
    "        # people_count = 0\n",
    "        # for box in r.boxes:\n",
    "        #     if r.names[int(box.cls)] == \"person\":\n",
    "        #         people_count += 1\n",
    "        \n",
    "        # total_people += people_count\n",
    "        # print(f\"{os.path.basename(path)}: {people_count} people\")\n",
    "        pass  # Remove this when you complete the TODO\n",
    "    \n",
    "    # TODO: Print total\n",
    "    # print(f\"\\nTotal people across all images: {total_people}\")\n",
    "    # return total_people\n",
    "\n",
    "# Test the function\n",
    "# images = [\n",
    "#     \"../images/yolo_beach_scene.jpg\",\n",
    "#     \"../images/yolo_traffic.jpg\",\n",
    "#     \"../images/yolo_einstein_head.jpg\",\n",
    "# ]\n",
    "# count_people_in_images(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
