{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Theory and Implementation with Keras\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are specialized neural networks designed for processing grid-structured data like images. Unlike fully connected networks that treat all pixels independently, CNNs exploit the spatial structure of images through local connectivity and weight sharing, making them far more efficient and effective for computer vision tasks.\n",
    "\n",
    "We will build and train CNN models using Keras on a flowers classification dataset, exploring core concepts like convolution, pooling, and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Dataset Loading](#Setup-and-Dataset-Loading)\n",
    "2. [Why CNNs for Images?](#Why-CNNs-for-Images?)\n",
    "3. [CNN Architecture Components](#CNN-Architecture-Components)\n",
    "4. [Building a CNN with Keras](#Building-a-CNN-with-Keras)\n",
    "5. [Training and Evaluating the Model](#Training-and-Evaluating-the-Model)\n",
    "6. [Data Augmentation](#Data-Augmentation)\n",
    "7. [Recap](#Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dataset Loading\n",
    "\n",
    "First, let's import the necessary libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Flowers Dataset\n",
    "\n",
    "We use a dataset of approximately 3,700 flower photographs from 5 different species. This is a moderate-sized dataset perfect for learning CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the flowers dataset\n",
    "dataset_url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "# Handle directory structure (Colab vs local)\n",
    "contents = os.listdir(data_dir)\n",
    "if 'flower_photos' in contents and len(contents) == 1:\n",
    "    data_dir = os.path.join(data_dir, 'flower_photos')\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "# Count total images\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(f'Total images: {image_count}')\n",
    "print(f'Categories: {sorted([item.name for item in data_dir.glob(\"*\") if item.is_dir()])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training and Validation Datasets\n",
    "\n",
    "We split the data into 80% training and 20% validation. Images are resized to 96×96 pixels for computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (96, 96)\n",
    "batch_size = 64\n",
    "\n",
    "# Create training dataset (80% of data)\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=42,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create validation dataset (20% of data)\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=42,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f'Classes: {class_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sample Images\n",
    "\n",
    "Let's look at a few examples from our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why CNNs for Images?\n",
    "\n",
    "### The Problem with Fully Connected Networks\n",
    "\n",
    "Consider using a standard fully connected (dense) neural network for image classification. A 96×96 RGB image has:\n",
    "\n",
    "- 96 × 96 × 3 = **27,648 input features**\n",
    "\n",
    "If we connect this to a hidden layer with just 1,000 neurons:\n",
    "\n",
    "- Number of parameters: 27,648 × 1,000 = **27.6 million parameters**\n",
    "\n",
    "This creates several problems:\n",
    "\n",
    "- **Too many parameters**: Difficult to train, requires massive amounts of data\n",
    "- **Ignores spatial structure**: Treats pixels at position (10, 10) and (90, 90) as completely unrelated\n",
    "- **Not translation invariant**: Must learn the same pattern (e.g., an edge) at every position independently\n",
    "\n",
    "### CNNs as the Solution\n",
    "\n",
    "CNNs address these issues through:\n",
    "\n",
    "- **Local connectivity**: Neurons connect only to a small region of the input\n",
    "- **Weight sharing**: Same filter applied across the entire image\n",
    "- **Translation invariance**: Features learned at one location work at any location\n",
    "\n",
    "This dramatically reduces parameters while improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture Components\n",
    "\n",
    "A typical CNN consists of several types of layers working together to extract and classify features.\n",
    "\n",
    "### 1. Convolutional Layers\n",
    "\n",
    "Convolutional layers are the core building blocks of CNNs. They apply learned filters (kernels) that slide across the input image, detecting features like edges, textures, or more complex patterns.\n",
    "\n",
    "**Key parameters:**\n",
    "- `filters`: Number of different feature maps to learn\n",
    "- `kernel_size`: Size of the sliding window (commonly 3×3 or 5×5)\n",
    "- `padding`: `'valid'` (no padding) or `'same'` (output same size as input)\n",
    "- `strides`: How far the filter moves (default is 1)\n",
    "- `activation`: Typically `'relu'` for non-linearity\n",
    "\n",
    "**Understanding Padding:**\n",
    "- **`padding='same'`**: Adds zeros around the input so output size equals input size. Example: 96×96 input with 3×3 kernel stays 96×96.\n",
    "- **`padding='valid'`**: No padding. Output shrinks. Example: 96×96 input with 3×3 kernel becomes 94×94.\n",
    "\n",
    "**Understanding Strides:**\n",
    "- **`strides=1`** (default): Filter moves 1 pixel at a time\n",
    "- **`strides=2`**: Filter jumps 2 pixels, halving output size. Example: 96×96 → 48×48 (faster than pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convolution with different padding\n",
    "sample_conv_same = layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')\n",
    "sample_conv_valid = layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='valid')\n",
    "\n",
    "# For a 96×96×3 input:\n",
    "# - 'same' padding: output is 96×96×32\n",
    "# - 'valid' padding: output is 94×94×32\n",
    "# - Parameters: (3×3×3 input channels) × 32 filters + 32 biases = 896 parameters\n",
    "print(f'Conv2D parameters: {(3*3*3) * 32 + 32}')\n",
    "print(\"With 'same' padding: 96×96 → 96×96\")\n",
    "print(\"With 'valid' padding: 96×96 → 94×94\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pooling Layers\n",
    "\n",
    "Pooling layers reduce spatial dimensions while retaining important features. This reduces computation, adds robustness to small translations, and helps prevent overfitting.\n",
    "\n",
    "**MaxPooling vs AveragePooling:**\n",
    "\n",
    "- **MaxPooling2D**: Takes the maximum value in each window\n",
    "  - Preserves strongest features (e.g., edges, important patterns)\n",
    "  - Most commonly used\n",
    "  - Best for feature detection tasks\n",
    "\n",
    "- **AveragePooling2D**: Computes the average value\n",
    "  - Smooths features\n",
    "  - Less common, but useful for reducing noise\n",
    "  - Best for smoother downsampling\n",
    "\n",
    "**Concrete Example:**\n",
    "\n",
    "Consider a 4×4 feature map with a 2×2 pooling window:\n",
    "\n",
    "```\n",
    "Input:          MaxPool Result:   AvgPool Result:\n",
    "[1  2  | 5  6]   [4  | 8]         [2.5 | 6.0]\n",
    "[3  4  | 7  8]   [---+---]        [----+----]\n",
    "[----+----]      [12 | 16]        [10.5| 14.5]\n",
    "[9  10 | 13 14]\n",
    "[11 12 | 15 16]\n",
    "```\n",
    "\n",
    "**GlobalAveragePooling2D:**\n",
    "- Reduces entire feature map (H×W) to a single value per channel\n",
    "- Example: 12×12×128 → 1×1×128 (then squeezed to 128)\n",
    "- Alternative to Flatten, reduces parameters significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate pooling operations\n",
    "# MaxPooling with 2×2 window reduces each dimension by half\n",
    "pool_layer = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "# For a 96×96 input, this produces a 48×48 output\n",
    "# Each 2×2 region becomes a single pixel (the maximum value)\n",
    "\n",
    "# GlobalAveragePooling alternative to Flatten\n",
    "# Reduces each entire feature map to one value\n",
    "global_pool = layers.GlobalAveragePooling2D()\n",
    "\n",
    "print(\"MaxPooling (2×2): 96×96 → 48×48 (keeps strongest features)\")\n",
    "print(\"AveragePooling (2×2): 96×96 → 48×48 (smooths features)\")\n",
    "print(\"GlobalAveragePooling: 12×12×128 → 128 (one value per channel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity, allowing the network to learn complex patterns.\n",
    "\n",
    "**ReLU (Rectified Linear Unit)** is the most common choice:\n",
    "- Formula: `f(x) = max(0, x)`\n",
    "- Advantages: Fast computation, helps avoid vanishing gradient, introduces sparsity\n",
    "- Applied after convolutional layers\n",
    "\n",
    "### 4. Flatten and Dense Layers\n",
    "\n",
    "After extracting features with convolutional and pooling layers:\n",
    "\n",
    "- **Flatten**: Converts 3D feature maps to 1D vector\n",
    "- **Dense layers**: Fully connected layers that learn combinations of features\n",
    "- **Output layer**: Dense layer with `softmax` activation for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After convolutional layers, we have a 3D tensor (batch, height, width, channels)\n",
    "# Flatten converts it to (batch, height × width × channels)\n",
    "\n",
    "flatten_layer = layers.Flatten()  # Converts 3D to 1D\n",
    "dense_layer = layers.Dense(64, activation='relu')  # Learns feature combinations\n",
    "output_layer = layers.Dense(5, activation='softmax')  # 5 classes, produces probabilities\n",
    "\n",
    "print(\"Flatten: 12×12×128 → 18,432 features\")\n",
    "print(\"Dense: Learns which feature combinations predict each class\")\n",
    "print(\"Output: Softmax produces class probabilities that sum to 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN with Keras\n",
    "\n",
    "Now we'll build a complete CNN architecture using the Functional API. Our model will have:\n",
    "\n",
    "1. Input layer (96×96×3)\n",
    "2. Rescaling layer (normalize pixel values to 0-1)\n",
    "3. Three convolutional blocks (Conv2D + MaxPooling)\n",
    "4. Flatten layer\n",
    "5. Dense layer\n",
    "6. Output layer (5 classes)\n",
    "\n",
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = keras.Input(shape=image_size + (3,), name='input')\n",
    "\n",
    "# Rescaling to [0, 1] - normalizes pixel values for better training\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "# First convolutional block: detect basic patterns (edges, colors)\n",
    "x = layers.Conv2D(32, 3, padding='same', activation='relu', name='conv_1')(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2), name='pool_1')(x)  # 96×96 → 48×48\n",
    "\n",
    "# Second convolutional block: detect more complex patterns (textures)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu', name='conv_2')(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2), name='pool_2')(x)  # 48×48 → 24×24\n",
    "\n",
    "# Third convolutional block: detect high-level patterns (petal shapes, flower parts)\n",
    "x = layers.Conv2D(128, 3, padding='same', activation='relu', name='conv_3')(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2), name='pool_3')(x)  # 24×24 → 12×12\n",
    "\n",
    "# Flatten 3D features to 1D for classification\n",
    "x = layers.Flatten(name='flatten')(x)\n",
    "\n",
    "# Dense layer learns combinations of features\n",
    "x = layers.Dense(128, activation='relu', name='dense')(x)\n",
    "\n",
    "# Output layer with 5 neurons (one per flower class)\n",
    "outputs = layers.Dense(len(class_names), activation='softmax', name='output')(x)\n",
    "\n",
    "# Create model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='flower_cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "Let's inspect the architecture and count parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- The spatial dimensions decrease progressively: 96×96 → 48×48 → 24×24 → 12×12\n",
    "- The number of feature maps increases: 32 → 64 → 128\n",
    "- Total parameters are much fewer than a fully connected network\n",
    "- Most parameters are in the dense layers, not the convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Data Shapes\n",
    "\n",
    "Verify the input data shape matches our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds.take(1):\n",
    "    print(f'Image batch shape: {image_batch.shape}')  # (batch_size, height, width, channels)\n",
    "    print(f'Labels batch shape: {labels_batch.shape}')  # (batch_size,)\n",
    "    print(f'\\nFirst image min/max values: {image_batch[0].numpy().min():.0f}/{image_batch[0].numpy().max():.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating the Model\n",
    "\n",
    "### Compiling the Model\n",
    "\n",
    "We configure the model for training with:\n",
    "- **Optimizer**: Adam (adaptive learning rate, works well by default)\n",
    "- **Loss**: Sparse categorical crossentropy (for integer labels, not one-hot)\n",
    "- **Metrics**: Accuracy (percentage of correct predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Train the model for a modest number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training History\n",
    "\n",
    "Plot training and validation accuracy and loss to understand model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Loss plot shows how well model fits the data\n",
    "    ax1.plot(hist['epoch'], hist['loss'], label='Train Loss', marker='o')\n",
    "    ax1.plot(hist['epoch'], hist['val_loss'], label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy plot shows prediction correctness\n",
    "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy', marker='o')\n",
    "    ax2.plot(hist['epoch'], hist['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "- If validation loss decreases with training loss: model is learning\n",
    "- If validation loss stops decreasing or increases: potential overfitting\n",
    "- Gap between training and validation accuracy indicates generalization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Validation Set\n",
    "\n",
    "Check final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = model.evaluate(val_ds, verbose=0)\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation artificially increases dataset diversity by applying random transformations to training images. This is crucial for preventing overfitting and improving generalization when training data is limited.\n",
    "\n",
    "### Why Data Augmentation?\n",
    "\n",
    "- **Prevents overfitting**: Model sees variations of each image, not just memorizing exact examples\n",
    "- **Improves generalization**: Learns to be invariant to rotations, flips, zooms, etc.\n",
    "- **Increases effective dataset size**: No need to collect more images\n",
    "\n",
    "### Common Augmentation Techniques\n",
    "\n",
    "Keras provides built-in augmentation layers:\n",
    "\n",
    "- **RandomFlip**: Horizontal/vertical mirroring (useful when orientation doesn't matter)\n",
    "- **RandomRotation**: Small rotations (realistic for flowers at different angles)\n",
    "- **RandomZoom**: Simulates different camera distances\n",
    "- **RandomContrast**: Handles varying lighting conditions\n",
    "\n",
    "### Creating an Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation pipeline - applied randomly during training only\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),  # Mirrors image\n",
    "    layers.RandomRotation(0.2),  # Rotate by ±20% of 360° = ±72°\n",
    "    layers.RandomZoom(0.2),      # Zoom in/out by ±20%\n",
    "    layers.RandomContrast(0.2),  # Adjust contrast for lighting variations\n",
    "], name='data_augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Augmented Images\n",
    "\n",
    "Let's see how augmentation transforms our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        # Apply augmentation - each call produces different transformations\n",
    "        augmented_images = data_augmentation(images, training=True)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.title(f'Augmented {i+1}')\n",
    "        plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Each transformation is applied randomly with some probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model with Augmentation\n",
    "\n",
    "We integrate augmentation into the model architecture. It's applied only during training, not during validation or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with augmentation at the beginning\n",
    "inputs = keras.Input(shape=image_size + (3,), name='input')\n",
    "\n",
    "# Data augmentation (only applied when training=True)\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# Rescaling\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "# Convolutional blocks (same architecture as before)\n",
    "x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Classifier\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "model_aug = keras.Model(inputs=inputs, outputs=outputs, name='flower_cnn_aug')\n",
    "\n",
    "# Compile\n",
    "model_aug.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "history_aug = model_aug.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history_aug)\n",
    "\n",
    "val_loss_aug, val_accuracy_aug = model_aug.evaluate(val_ds, verbose=0)\n",
    "print(f'\\nValidation Results:')\n",
    "print(f'Without augmentation: {val_accuracy:.4f}')\n",
    "print(f'With augmentation: {val_accuracy_aug:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected observations:**\n",
    "\n",
    "- Training may be slower (more data variations to process)\n",
    "- Training accuracy might be lower (task is harder with augmentation)\n",
    "- Validation accuracy often improves (better generalization)\n",
    "- Smaller gap between training and validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "**Why CNNs:**\n",
    "- Fully connected networks have too many parameters for images\n",
    "- CNNs use local connectivity and weight sharing for efficiency\n",
    "- Translation invariance makes CNNs effective for vision tasks\n",
    "\n",
    "**CNN Components:**\n",
    "- **Conv2D**: Extracts features using learned filters\n",
    "- **MaxPooling**: Reduces spatial dimensions, keeps strongest features\n",
    "- **Padding='same'**: Maintains spatial dimensions\n",
    "- **Stride**: Controls filter movement and output size\n",
    "- **ReLU**: Introduces non-linearity\n",
    "- **Dense layers**: Learns feature combinations for classification\n",
    "\n",
    "**Keras Workflow:**\n",
    "- Functional API for flexible model building\n",
    "- `model.summary()` shows architecture and parameters\n",
    "- `compile()`, `fit()`, `evaluate()` for training\n",
    "\n",
    "**Data Augmentation:**\n",
    "- Random transformations prevent overfitting\n",
    "- Applied during training only (not validation/prediction)\n",
    "- Improves generalization with limited data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
