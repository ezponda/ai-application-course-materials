{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation and Pose Estimation with Ultralytics YOLO\n",
    "\n",
    "In this notebook we go beyond bounding boxes and explore two advanced vision tasks supported by Ultralytics YOLO models:\n",
    "- **Instance segmentation** (pixel-level object masks)\n",
    "- **Pose estimation** (keypoints on human bodies)\n",
    "\n",
    "We will use the same YOLO11 family of models and the example images downloaded in the Ultralytics introduction notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Models](#Setup-and-Models)\n",
    "2. [What Is Instance Segmentation?](#What-Is-Instance-Segmentation?)\n",
    "3. [Segmentation with YOLO11](#Segmentation-with-YOLO11)\n",
    "4. [Binary Masks and Pixel Operations](#Binary-Masks-and-Pixel-Operations)\n",
    "5. [Exercise: Pixelate a Person Using a Segmentation Mask](#Exercise-Pixelate-a-Person-Using-a-Segmentation-Mask)\n",
    "6. [What Is Pose Estimation?](#What-Is-Pose-Estimation?)\n",
    "7. [Pose Estimation with YOLO11](#Pose-Estimation-with-YOLO11)\n",
    "8. [Exercise: Explore Keypoints on Yoga Images](#Exercise-Explore-Keypoints-on-Yoga-Images)\n",
    "9. [Recap](#Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Models\n",
    "\n",
    "Let's import the necessary libraries. Make sure to run `07_intro_to_ultralytics.ipynb` first to download the example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Check that images are available\n",
    "if not os.path.exists(\"../images\"):\n",
    "    print(\"Warning: ../images/ not found. Run 07_intro_to_ultralytics.ipynb first to download example images.\")\n",
    "else:\n",
    "    print(\"Images directory found. Ready to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Instance Segmentation?\n",
    "\n",
    "Instance segmentation combines object detection with semantic segmentation. Instead of just drawing a bounding box around an object, it identifies the exact pixels that belong to each object instance.\n",
    "\n",
    "### Detection vs. Segmentation\n",
    "\n",
    "- **Object Detection**: Outputs bounding boxes (rectangles) around objects\n",
    "  - Fast and efficient\n",
    "  - Good for counting and localization\n",
    "  - Less precise (includes background pixels)\n",
    "\n",
    "- **Instance Segmentation**: Outputs pixel-level masks for each object\n",
    "  - More computationally expensive\n",
    "  - Precise object boundaries\n",
    "  - Useful for applications requiring exact shapes (medical imaging, video editing)\n",
    "\n",
    "### Semantic vs. Instance Segmentation\n",
    "\n",
    "- **Semantic Segmentation**: Labels every pixel with a class (all cars are \"car\")\n",
    "- **Instance Segmentation**: Distinguishes between different instances of the same class (car #1, car #2, etc.)\n",
    "\n",
    "YOLO models with the `-seg` suffix perform instance segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation with YOLO11\n",
    "\n",
    "Let's load a segmentation model and run it on one of our example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO11 segmentation model (nano version)\n",
    "seg_model = YOLO(\"yolo11n-seg.pt\")\n",
    "\n",
    "print(f\"Segmentation model loaded: {seg_model.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run segmentation on a traffic image\n",
    "image_path = \"../images/yolo_traffic.jpg\"\n",
    "img_bgr = cv2.imread(image_path)\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = seg_model(img_rgb)\n",
    "result = results[0]\n",
    "annotated = result.plot()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.imshow(annotated)\n",
    "plt.title(\"YOLO11 Instance Segmentation\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we see:**\n",
    "\n",
    "- Colored masks showing the exact shape of each object\n",
    "- Different colors for different instances (even of the same class)\n",
    "- Bounding boxes and class labels overlaid on the masks\n",
    "- Much more precise than simple rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the segmentation results\n",
    "print(\"Segmentation results structure:\")\n",
    "print(f\"  Number of detected objects: {len(result.boxes)}\")\n",
    "print(f\"  Masks available: {result.masks is not None}\")\n",
    "\n",
    "if result.masks is not None:\n",
    "    print(f\"  Number of masks: {len(result.masks)}\")\n",
    "    print(f\"  Mask data shape: {result.masks.data.shape}\")\n",
    "\n",
    "# List detected objects\n",
    "print(\"\\nDetected objects:\")\n",
    "for i, box in enumerate(result.boxes):\n",
    "    cls_id = int(box.cls)\n",
    "    cls_name = result.names[cls_id]\n",
    "    conf = float(box.conf)\n",
    "    print(f\"  {i+1}. {cls_name} (conf: {conf:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Masks and Pixel Operations\n",
    "\n",
    "A **binary mask** is a 2D array where pixels are either `True` (object) or `False` (background). We can use masks to:\n",
    "\n",
    "- Extract only the object (remove background)\n",
    "- Blur or pixelate specific regions\n",
    "- Anonymize faces or people for privacy\n",
    "- Replace backgrounds in images\n",
    "\n",
    "Let's create a helper function to extract boolean masks from YOLO segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.engine.results import Results\n",
    "\n",
    "def extract_segmentation_mask(detection_result: Results, object_index: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract a boolean mask for one detected instance from YOLO segmentation results.\n",
    "    \n",
    "    Args:\n",
    "        detection_result: YOLO Results object containing segmentation data\n",
    "        object_index: Index of the object to extract (0 for first detection)\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask as a boolean NumPy array (True for object pixels, False for background)\n",
    "    \"\"\"\n",
    "    # Verify the inputs\n",
    "    if detection_result.masks is None:\n",
    "        raise ValueError(\"No segmentation masks found in detection results\")\n",
    "    \n",
    "    if object_index >= len(detection_result.masks.xy):\n",
    "        raise IndexError(f\"Object index {object_index} is out of range. \"\n",
    "                         f\"Only {len(detection_result.masks.xy)} objects detected.\")\n",
    "    \n",
    "    # Get original image dimensions\n",
    "    height, width = detection_result.orig_img.shape[:2]\n",
    "    \n",
    "    # Create an empty mask with the original image dimensions\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Get the contour points from the detection result\n",
    "    # This can be either a single polygon (2D array) or a list of polygons\n",
    "    polygon_data = detection_result.masks.xy[object_index]\n",
    "    \n",
    "    # Handle both cases: single polygon or list of polygons\n",
    "    if isinstance(polygon_data, np.ndarray) and polygon_data.ndim == 2:\n",
    "        # Single polygon: shape (N, 2)\n",
    "        contours = [polygon_data.astype(np.int32).reshape(-1, 1, 2)]\n",
    "    elif isinstance(polygon_data, list):\n",
    "        # List of polygons\n",
    "        contours = [p.astype(np.int32).reshape(-1, 1, 2) for p in polygon_data]\n",
    "    else:\n",
    "        # Fallback: treat as single polygon\n",
    "        contours = [np.array(polygon_data).astype(np.int32).reshape(-1, 1, 2)]\n",
    "    \n",
    "    # Fill all polygons into the mask\n",
    "    cv2.fillPoly(mask, contours, 255)\n",
    "    \n",
    "    # Convert to boolean mask\n",
    "    binary_mask = mask.astype(bool)\n",
    "    \n",
    "    return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate extracting and visualizing a mask\n",
    "# Use Einstein image for a clearer example\n",
    "einstein_path = \"../images/yolo_einstein_head.jpg\"\n",
    "einstein_bgr = cv2.imread(einstein_path)\n",
    "einstein_rgb = cv2.cvtColor(einstein_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run segmentation\n",
    "einstein_results = seg_model(einstein_rgb, conf=0.3)\n",
    "einstein_result = einstein_results[0]\n",
    "\n",
    "# Show default visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(einstein_rgb)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(einstein_result.plot())\n",
    "plt.title(\"Segmentation Result\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of objects detected: {len(einstein_result.boxes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize the binary mask\n",
    "if einstein_result.masks is not None and len(einstein_result.masks) > 0:\n",
    "    # Extract mask for the first detected object\n",
    "    binary_mask = extract_segmentation_mask(einstein_result, 0)\n",
    "    \n",
    "    # Visualize the mask\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(einstein_rgb)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Binary mask\n",
    "    axes[1].imshow(binary_mask, cmap=\"gray\")\n",
    "    axes[1].set_title(\"Binary Mask\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    # Masked image (object only)\n",
    "    masked_img = einstein_rgb.copy()\n",
    "    masked_img[~binary_mask] = 0  # Set background to black\n",
    "    axes[2].imshow(masked_img)\n",
    "    axes[2].set_title(\"Extracted Object\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No masks detected. Try with a different image or lower confidence threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate mask overlay: tint the detected object in red\n",
    "if einstein_result.masks is not None and len(einstein_result.masks) > 0:\n",
    "    overlay = einstein_rgb.copy()\n",
    "    overlay[binary_mask] = [255, 0, 0]  # Set object pixels to red\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(einstein_rgb)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(\"Object Highlighted (Red Overlay)\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Pixelate a Person Using a Segmentation Mask\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Detect a person in an image\n",
    "2. Build a binary mask for that person\n",
    "3. Pixelate either the person or the background\n",
    "\n",
    "This technique is commonly used for **privacy protection** and **data anonymization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise: Pixelate a person using a segmentation mask\n\n# Load an image with people (beach scene or traffic)\nimage_path = \"../images/yolo_beach_scene.jpg\"\nimg_bgr = cv2.imread(image_path)\nimg_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n\n# Run segmentation\nseg_model = YOLO(\"yolo11n-seg.pt\")\nresults = seg_model(img_rgb, conf=0.25)\nresult = results[0]\n\n# TODO: Find the class ID for 'person' in result.names\n# Hint: result.names is a dictionary like {0: 'person', 1: 'bicycle', ...}\n# Hint: Loop through result.names.items() and check for 'person'\n\n# TODO: Find the first detection that corresponds to a person\n# Hint: Loop through result.boxes and check if int(box.cls) matches the person class ID\n\n# TODO: Build a binary mask for that person\n# Hint: Use extract_segmentation_mask(result, object_index=person_index)\n\n# TODO: Create pixelated variants\n# Hint: You'll need a pixelate() function (define one or use cv2.resize with INTER_NEAREST)\n# Hint: Use boolean indexing with the mask: result_img[person_mask] = pixelated[person_mask]\n\n# TODO: Plot results in a 2x2 grid showing:\n# - Original image\n# - Fully pixelated\n# - Pixelated background (clear person)\n# - Pixelated person (clear background)\n\npass  # Remove this line when you complete the TODOs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use cases for mask-based pixelation:**\n",
    "\n",
    "- **Privacy protection**: Anonymize faces or people in public datasets\n",
    "- **Content moderation**: Blur sensitive content automatically\n",
    "- **Data compliance**: Meet GDPR or other privacy regulations\n",
    "- **Creative effects**: Highlight subjects by blurring backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Pose Estimation?\n",
    "\n",
    "Pose estimation detects human figures and identifies key body points (keypoints). These keypoints typically include:\n",
    "\n",
    "- **Face**: Nose, eyes, ears\n",
    "- **Upper body**: Shoulders, elbows, wrists\n",
    "- **Lower body**: Hips, knees, ankles\n",
    "\n",
    "### What the Model Outputs\n",
    "\n",
    "For each detected person:\n",
    "- **Bounding box**: Location of the person\n",
    "- **Keypoints**: (x, y) coordinates for each body point\n",
    "- **Confidence**: How certain the model is about each keypoint\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Fitness tracking**: Analyze exercise form\n",
    "- **Sports analytics**: Track athlete movements\n",
    "- **Human-computer interaction**: Gesture recognition\n",
    "- **Animation**: Motion capture for games and movies\n",
    "- **Surveillance**: Detect abnormal behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimation with YOLO11\n",
    "\n",
    "Let's load a pose estimation model and run it on yoga images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO11 pose estimation model\n",
    "pose_model = YOLO(\"yolo11n-pose.pt\")\n",
    "\n",
    "print(f\"Pose model loaded: {pose_model.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pose estimation on yoga images\n",
    "yoga_images = [\n",
    "    \"../images/yolo_yoga_1.jpg\",\n",
    "    \"../images/yolo_yoga_2.jpg\",\n",
    "]\n",
    "\n",
    "for path in yoga_images:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: {path} not found. Run 07_intro_to_ultralytics.ipynb first.\")\n",
    "        continue\n",
    "    \n",
    "    img_bgr = cv2.imread(path)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    results = pose_model(img_rgb)\n",
    "    r = results[0]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(r.plot())\n",
    "    plt.title(f\"Pose Estimation: {os.path.basename(path)}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we see:**\n",
    "\n",
    "- Keypoints drawn as circles on body joints\n",
    "- Lines (skeleton) connecting related keypoints\n",
    "- Different colors for different body parts\n",
    "- Bounding boxes around detected people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the keypoints structure\n",
    "if r.keypoints is not None:\n",
    "    print(\"Keypoints data structure:\")\n",
    "    print(f\"  Shape: {r.keypoints.data.shape}\")\n",
    "    print(f\"  Interpretation: (num_people, num_keypoints, coordinates)\")\n",
    "    print(f\"  Coordinates: [x, y, confidence] for each keypoint\")\n",
    "    \n",
    "    if r.keypoints.conf is not None:\n",
    "        print(f\"\\nKeypoint confidences shape: {r.keypoints.conf.shape}\")\n",
    "        print(f\"  Interpretation: (num_people, num_keypoints)\")\n",
    "        \n",
    "        # Show confidence for first person\n",
    "        if len(r.keypoints.conf) > 0:\n",
    "            first_person_conf = r.keypoints.conf[0].cpu().numpy()\n",
    "            print(f\"\\nFirst person keypoint confidences:\")\n",
    "            print(f\"  Mean: {np.mean(first_person_conf):.3f}\")\n",
    "            print(f\"  Min: {np.min(first_person_conf):.3f}\")\n",
    "            print(f\"  Max: {np.max(first_person_conf):.3f}\")\n",
    "else:\n",
    "    print(\"No keypoints detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise: Explore Keypoints on Yoga Images\n\nAnalyze keypoint detection results by computing statistics on the model's confidence for each person."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise: Analyze keypoints on yoga images\n\ndef summarize_pose_results(result):\n    \"\"\"\n    Print a short summary of pose estimation results:\n    - Number of people detected\n    - Average keypoint confidence per person\n    \"\"\"\n    if result.keypoints is None:\n        print(\"No keypoints found.\")\n        return\n    \n    # TODO: Extract keypoint confidence tensor\n    # Hint: result.keypoints.conf has shape (num_people, num_keypoints)\n    \n    # TODO: Compute average confidence per person\n    # Hint: Use .mean(axis=1) to average across keypoints for each person\n    # Hint: Convert to numpy with .cpu().numpy()\n    \n    # TODO: Print a short summary\n    # Hint: Print the number of people and average confidence for each\n    \n    pass  # Remove this line when you complete the TODOs\n\n# Test on yoga images\nfor path in yoga_images:\n    if not os.path.exists(path):\n        continue\n    \n    img_bgr = cv2.imread(path)\n    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n    \n    results = pose_model(img_rgb)\n    r = results[0]\n    \n    print(f\"\\n=== {os.path.basename(path)} ===\")\n    summarize_pose_results(r)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Recap\n\n### Key Takeaways\n\n- **Instance Segmentation** provides pixel-level masks for each object, not just bounding boxes\n- **Binary masks** (0/1 arrays) can be used to isolate objects, blur regions, or anonymize content\n- **Pose Estimation** detects human keypoints (joints) and their connections (skeleton)\n- **Keypoint confidence** tells us how certain the model is about each body joint\n- **Ultralytics API** is consistent: `YOLO(\"yolo11n-seg.pt\")` for segmentation, `YOLO(\"yolo11n-pose.pt\")` for pose\n- **Applications**: Privacy protection, content moderation, fitness tracking, sports analytics, motion capture"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}