{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Tracking with Ultralytics YOLO\n",
    "\n",
    "In this notebook we extend object detection to **object tracking** using Ultralytics YOLO11. We will see how to follow objects across frames, understand track IDs, and implement simple object counting in a traffic video.\n",
    "\n",
    "> **Note:** We use the Ultralytics YOLO family (v3â€“v11 and newer). In this notebook we demonstrate with YOLO11 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#Setup-and-Imports)\n",
    "2. [Object Tracking vs Object Detection](#Object-Tracking-vs-Object-Detection)\n",
    "3. [Tracking with Ultralytics YOLO](#Tracking-with-Ultralytics-YOLO)\n",
    "4. [Track IDs and Common Challenges](#Track-IDs-and-Common-Challenges)\n",
    "5. [Video Tracking Demo](#Video-Tracking-Demo)\n",
    "6. [Counting Objects with a Region of Interest (ROI)](#Counting-Objects-with-a-Region-of-Interest-(ROI))\n",
    "7. [Exercises: Unique IDs and Lane-wise Counts](#Exercises-Unique-IDs-and-Lane-wise-Counts)\n",
    "8. [Recap](#Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install in fresh environments\n",
    "# %pip install ultralytics opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Fix for potential library conflicts on some systems\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# Load YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "print(f\"Model loaded: {model.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for displaying frames\n",
    "def show_frame(img, roi=None, window_name=\"YOLO Tracking\"):\n",
    "    \"\"\"\n",
    "    Display an image frame with optional ROI rectangle.\n",
    "    Works in both Colab and local environments.\n",
    "    \"\"\"\n",
    "    if roi is not None:\n",
    "        x1, y1, x2, y2 = roi\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "    \n",
    "    try:\n",
    "        # Colab environment\n",
    "        from google.colab.patches import cv2_imshow\n",
    "        from IPython.display import clear_output\n",
    "        clear_output(wait=True)\n",
    "        cv2_imshow(img)\n",
    "    except ImportError:\n",
    "        # Local environment\n",
    "        cv2.imshow(window_name, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Tracking vs Object Detection\n",
    "\n",
    "**Object detection** identifies and localizes objects in a single image frame, outputting bounding boxes and class labels. Each frame is processed independently.\n",
    "\n",
    "**Object tracking** extends detection by maintaining consistent identifiers for each object across multiple frames. This allows us to follow individual objects over time, which is essential for:\n",
    "\n",
    "- Counting vehicles passing through a road\n",
    "- Monitoring customer flow in retail stores\n",
    "- Analyzing player movements in sports\n",
    "- Surveillance and security applications\n",
    "\n",
    "### Tracking-by-Detection Paradigm\n",
    "\n",
    "Modern tracking systems like Ultralytics YOLO use a **tracking-by-detection** approach:\n",
    "\n",
    "1. An object detector runs on each frame independently\n",
    "2. A tracking algorithm associates detections across consecutive frames\n",
    "3. Each object receives a persistent ID that follows it through the video\n",
    "\n",
    "This is different from classical single-object trackers that use correlation filters or template matching to follow one specific target, or optical flow methods that track pixel movements between frames.\n",
    "\n",
    "### Challenges in Object Tracking\n",
    "\n",
    "- **Occlusion**: Objects may temporarily disappear behind other objects\n",
    "- **Objects entering/leaving**: New objects appear, others exit the frame\n",
    "- **ID switches**: The tracker may incorrectly swap IDs between similar objects\n",
    "- **Missed detections**: Objects may not be detected in some frames\n",
    "- **Appearance changes**: Lighting, angle, or scale changes can confuse the tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking with Ultralytics YOLO\n",
    "\n",
    "Ultralytics provides built-in tracking capabilities. Instead of `model(image)`, we use:\n",
    "\n",
    "```python\n",
    "results = model.track(frame, persist=True)\n",
    "```\n",
    "\n",
    "Key parameters:\n",
    "- **`persist=True`**: Maintain track history across frames\n",
    "- **`tracker=\"botsort.yaml\"`**: Use BoT-SORT tracker (default)\n",
    "- **`tracker=\"bytetrack.yaml\"`**: Use ByteTrack tracker\n",
    "\n",
    "### Tracking Algorithms in Ultralytics\n",
    "\n",
    "Ultralytics supports several tracking algorithms that differ in how they associate detections across frames:\n",
    "\n",
    "**SORT / DeepSORT**\n",
    "- Uses Kalman filter to predict object positions and Hungarian algorithm for assignment\n",
    "- Simple, fast, and works well when detections are stable\n",
    "- DeepSORT adds appearance features (re-identification) for better association\n",
    "\n",
    "**ByteTrack**\n",
    "- Uses both high-confidence and low-confidence detections\n",
    "- Low-score detections help maintain tracks during partial occlusions\n",
    "- Particularly effective in crowded scenes with many overlapping objects\n",
    "\n",
    "**BoT-SORT** (default in Ultralytics)\n",
    "- Combines motion prediction with appearance features\n",
    "- More robust to occlusions and short-term disappearances\n",
    "- Good balance between speed and accuracy\n",
    "\n",
    "Example with different trackers:\n",
    "\n",
    "```python\n",
    "# Default BoT-SORT\n",
    "results = model.track(frame, persist=True, tracker=\"botsort.yaml\")\n",
    "\n",
    "# ByteTrack for dense scenes\n",
    "results = model.track(frame, persist=True, tracker=\"bytetrack.yaml\")\n",
    "```\n",
    "\n",
    "Let's start with a single frame from our traffic video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic video URL\n",
    "traffic_url = \"https://github.com/ezponda/intro_deep_learning/raw/main/images/road_traffic_video_for_object_recognition_short.mp4\"\n",
    "\n",
    "# Extract one frame for demonstration\n",
    "cap = cv2.VideoCapture(traffic_url)\n",
    "success, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if success:\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(frame_rgb)\n",
    "    plt.title(\"Sample frame from traffic video\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to read video. Check the URL or network connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tracking on the single frame\n",
    "results = model.track(frame, persist=True, verbose=False)\n",
    "result = results[0]\n",
    "\n",
    "# Display annotated frame\n",
    "annotated = result.plot()\n",
    "annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(annotated_rgb)\n",
    "plt.title(\"First tracking result (with track IDs)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the tracking results\n",
    "print(\"Tracking results structure:\")\n",
    "print(f\"  Classes detected: {result.boxes.cls.cpu().numpy()}\")\n",
    "print(f\"  Track IDs: {result.boxes.id.cpu().numpy() if result.boxes.id is not None else 'None'}\")\n",
    "print(f\"  Class names: {result.names}\")\n",
    "\n",
    "# List detections with their track IDs\n",
    "print(\"\\nDetected objects:\")\n",
    "for i, (cls_id, track_id) in enumerate(zip(result.boxes.cls, result.boxes.id)):\n",
    "    cls_name = result.names[int(cls_id)]\n",
    "    print(f\"  {i+1}. {cls_name} (Track ID: {int(track_id)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track IDs and Common Challenges\n",
    "\n",
    "### What are Track IDs?\n",
    "\n",
    "Each detected object receives a unique **track ID** that persists across frames. This ID allows us to:\n",
    "\n",
    "- Count unique objects (not just total detections)\n",
    "- Measure how long an object stays in view\n",
    "- Analyze movement patterns and trajectories\n",
    "\n",
    "### Accessing Track IDs\n",
    "\n",
    "```python\n",
    "result.boxes.id  # Tensor of track IDs (one per detection)\n",
    "result.boxes.cls  # Class indices\n",
    "result.boxes.xyxy  # Bounding box coordinates\n",
    "```\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "- **ID switches**: When two objects cross paths, the tracker may swap their IDs\n",
    "- **Lost tracks**: If an object disappears for too long, it may get a new ID when it reappears\n",
    "- **Fragmented tracks**: Same object might have multiple IDs if detection is intermittent\n",
    "\n",
    "These issues are inherent to tracking algorithms and can be mitigated by tuning parameters or using more sophisticated trackers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Tracking Demo\n",
    "\n",
    "Let's run tracking on the entire traffic video. The loop processes each frame, maintains track history, and displays the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full video tracking loop\n",
    "cap = cv2.VideoCapture(traffic_url)\n",
    "\n",
    "print(\"Running video tracking... Press 'q' to stop early.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    # Run tracking with persistence\n",
    "    results = model.track(frame, persist=True, verbose=False)\n",
    "    result = results[0]\n",
    "    \n",
    "    # Visualize results\n",
    "    annotated = result.plot()\n",
    "    show_frame(annotated)\n",
    "    \n",
    "    # Exit on 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Video tracking completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Webcam Tracking\n",
    "\n",
    "If you have a webcam, you can try real-time tracking. This works in local environments but not in hosted notebooks like Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Webcam tracking (uncomment to use)\n",
    "# Requires local environment with webcam access\n",
    "\n",
    "# cap = cv2.VideoCapture(0)  # 0 = default webcam\n",
    "# print(\"Webcam tracking... Press 'q' to stop.\")\n",
    "#\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     \n",
    "#     results = model.track(frame, persist=True, verbose=False)\n",
    "#     annotated = results[0].plot()\n",
    "#     \n",
    "#     cv2.imshow(\"Webcam Tracking\", annotated)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "#\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Webcam code is commented out. Uncomment to try real-time tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Objects with a Region of Interest (ROI)\n",
    "\n",
    "A common use case for tracking is counting objects that pass through a specific area. We define a **Region of Interest (ROI)** as a rectangle and count unique objects whose centers fall within it.\n",
    "\n",
    "### Why Use an ROI?\n",
    "\n",
    "- Count vehicles crossing a specific road section\n",
    "- Monitor entries/exits at a gate or door\n",
    "- Focus on relevant areas while ignoring distractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleObjectCounter:\n",
    "    \"\"\"\n",
    "    Count unique objects passing through a region of interest.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, roi, class_names=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            roi: Tuple (x1, y1, x2, y2) defining the rectangular region\n",
    "            class_names: List of class names to count, or None for all classes\n",
    "        \"\"\"\n",
    "        self.roi = roi\n",
    "        self.class_names = class_names\n",
    "        self.unique_objects = {}  # {class_name: set of track_ids}\n",
    "    \n",
    "    def is_in_roi(self, box):\n",
    "        \"\"\"Check if box center is within ROI.\"\"\"\n",
    "        x_center = (box[0] + box[2]) / 2\n",
    "        y_center = (box[1] + box[3]) / 2\n",
    "        return (self.roi[0] < x_center < self.roi[2] and \n",
    "                self.roi[1] < y_center < self.roi[3])\n",
    "    \n",
    "    def update(self, result):\n",
    "        \"\"\"Update counts based on tracking results.\"\"\"\n",
    "        if result.boxes.id is None:\n",
    "            return\n",
    "        \n",
    "        for i in range(len(result.boxes)):\n",
    "            box = result.boxes.xyxy[i].cpu().numpy()\n",
    "            track_id = int(result.boxes.id[i])\n",
    "            cls_name = result.names[int(result.boxes.cls[i])]\n",
    "            \n",
    "            if self.is_in_roi(box):\n",
    "                if self.class_names is None or cls_name in self.class_names:\n",
    "                    if cls_name not in self.unique_objects:\n",
    "                        self.unique_objects[cls_name] = set()\n",
    "                    self.unique_objects[cls_name].add(track_id)\n",
    "    \n",
    "    def get_counts(self):\n",
    "        \"\"\"Return dictionary of counts per class.\"\"\"\n",
    "        return {cls: len(ids) for cls, ids in self.unique_objects.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ROI and classes to count\n",
    "roi = (100, 150, 500, 350)  # (x1, y1, x2, y2)\n",
    "classes_to_count = ['truck', 'car', 'bus']\n",
    "\n",
    "counter = SimpleObjectCounter(roi, classes_to_count)\n",
    "\n",
    "# Run tracking with counting\n",
    "cap = cv2.VideoCapture(traffic_url)\n",
    "print(f\"Counting objects in ROI: {roi}\")\n",
    "print(f\"Classes: {classes_to_count}\")\n",
    "print(\"Running... Press 'q' to stop early.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    results = model.track(frame, persist=True, verbose=False)\n",
    "    result = results[0]\n",
    "    \n",
    "    # Update counter\n",
    "    counter.update(result)\n",
    "    \n",
    "    # Display with ROI\n",
    "    annotated = result.plot()\n",
    "    \n",
    "    # Add count overlay\n",
    "    counts = counter.get_counts()\n",
    "    y_pos = 30\n",
    "    for cls_name, count in counts.items():\n",
    "        text = f\"{cls_name}: {count}\"\n",
    "        cv2.putText(annotated, text, (10, y_pos), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        y_pos += 30\n",
    "    \n",
    "    show_frame(annotated, roi)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\nFinal counts:\")\n",
    "for cls_name, count in counter.get_counts().items():\n",
    "    print(f\"  {cls_name}: {count} unique objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Unique IDs and Lane-wise Counts\n",
    "\n",
    "### Exercise 1: Count Unique Trucks\n",
    "\n",
    "Count the number of **unique trucks** (by track ID) that appear anywhere in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Count unique trucks in the video\n",
    "\n",
    "cap = cv2.VideoCapture(traffic_url)\n",
    "unique_truck_ids = set()\n",
    "\n",
    "print(\"Counting unique trucks... Press 'q' to stop early.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    results = model.track(frame, persist=True, verbose=False)\n",
    "    result = results[0]\n",
    "    \n",
    "    # TODO: Loop over detections and add truck track IDs to unique_truck_ids\n",
    "    # Hint: Use result.names to map class IDs to names (e.g., 'truck')\n",
    "    # Hint: Use result.boxes.id to get track IDs\n",
    "    # Hint: Check if result.boxes.id is not None before accessing it\n",
    "    \n",
    "    pass  # Your code here\n",
    "    \n",
    "    annotated = result.plot()\n",
    "    show_frame(annotated)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"\\nTotal unique trucks: {len(unique_truck_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Lane-wise Truck Counts\n",
    "\n",
    "Count trucks passing through two different lanes (left and right) by defining two separate ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, visualize the last frame to help define ROIs\n",
    "cap = cv2.VideoCapture(traffic_url)\n",
    "last_frame = None\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    last_frame = frame\n",
    "cap.release()\n",
    "\n",
    "if last_frame is not None:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.imshow(cv2.cvtColor(last_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Reference frame for defining lane ROIs\")\n",
    "    plt.axis(\"on\")  # Keep axis to see coordinates\n",
    "    plt.show()\n",
    "    print(f\"Frame size: {last_frame.shape[1]} x {last_frame.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Count trucks per lane\n",
    "\n",
    "# TODO: Define ROIs for left and right lanes based on the reference frame above\n",
    "# Hint: ROI format is (x1, y1, x2, y2) where (x1, y1) is top-left corner\n",
    "left_lane_roi = None  # Replace with actual coordinates\n",
    "right_lane_roi = None  # Replace with actual coordinates\n",
    "\n",
    "# TODO: Create two SimpleObjectCounter instances, one for each lane\n",
    "# Hint: SimpleObjectCounter(roi, ['truck'])\n",
    "left_counter = None  # Your counter here\n",
    "right_counter = None  # Your counter here\n",
    "\n",
    "# TODO: Run tracking loop and update both counters\n",
    "# Hint: Call counter.update(result) for each counter\n",
    "# Hint: Draw both ROIs on the annotated frame using cv2.rectangle\n",
    "\n",
    "pass  # Your tracking loop here\n",
    "\n",
    "# TODO: Print final counts\n",
    "# Hint: Use counter.get_counts().get('truck', 0) to get truck count\n",
    "# Calculate and print: left lane trucks, right lane trucks, difference\n",
    "\n",
    "pass  # Your print statements here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Object tracking** extends detection by maintaining consistent IDs across frames\n",
    "- **Tracking-by-detection** runs a detector on each frame and associates detections over time\n",
    "- **Track IDs** (`result.boxes.id`) uniquely identify each object throughout a video\n",
    "- **`model.track(..., persist=True)`** maintains tracking history between calls\n",
    "- **Tracking algorithms** (BoT-SORT, ByteTrack) differ in how they handle occlusions and associations\n",
    "- **Region of Interest (ROI)** allows focused counting in specific image areas\n",
    "- **Unique object counting** uses track IDs to avoid double-counting the same object\n",
    "- **Common challenges**: Occlusion, ID switches, missed detections, objects entering/leaving the scene"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
