{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning and Fine-tuning\n",
    "\n",
    "Transfer learning leverages knowledge from models pre-trained on large datasets to solve new tasks with less data and training time. Instead of training a CNN from scratch, we reuse learned features from models trained on millions of images, then adapt them to our specific problem. This approach often achieves better results with significantly less computational cost.\n",
    "\n",
    "We will use pre-trained models from Keras Applications to classify flower images, demonstrating both feature extraction and fine-tuning strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Data Preparation](#Setup-and-Data-Preparation)\n",
    "2. [What is Transfer Learning?](#What-is-Transfer-Learning?)\n",
    "3. [Pre-trained Models in Keras](#Pre-trained-Models-in-Keras)\n",
    "4. [Feature Extraction with Frozen Backbone](#Feature-Extraction-with-Frozen-Backbone)\n",
    "5. [Combining Custom CNN with Pre-trained Features](#Combining-Custom-CNN-with-Pre-trained-Features)\n",
    "6. [Fine-tuning](#Fine-tuning)\n",
    "7. [Comparing Approaches](#Comparing-Approaches)\n",
    "8. [Recap](#Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Preparation\n",
    "\n",
    "First, let's set up our environment and load the same flowers dataset we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Flowers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset (~218MB)\n",
    "dataset_url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "# Handle potential nested directory structure\n",
    "contents = os.listdir(data_dir)\n",
    "if 'flower_photos' in contents and len(contents) == 1:\n",
    "    data_dir = os.path.join(data_dir, 'flower_photos')\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "print(f'Total images: {len(list(data_dir.glob(\"*/*.jpg\")))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Datasets\n",
    "\n",
    "We use the same 96\u00d796 image size and 80/20 train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (96, 96)\n",
    "batch_size = 64\n",
    "\n",
    "# Create training dataset (80% of data)\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=42,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create validation dataset (20% of data)\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=42,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(f'Classes: {class_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Performance Optimization\n",
    "\n",
    "Configure datasets for efficient training with caching and prefetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize dataset performance - prevents GPU from waiting for data\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print('Dataset optimization applied')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Transfer Learning?\n",
    "\n",
    "Transfer learning is the practice of using knowledge learned from one task to improve performance on a related task.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Instead of training a CNN from scratch:\n",
    "\n",
    "1. **Start with a pre-trained model**: Trained on a large dataset (e.g., ImageNet with 1.4M images)\n",
    "2. **Reuse learned features**: Early layers detect edges, textures, shapes\u2014useful for many vision tasks\n",
    "3. **Adapt to new task**: Replace or retrain only the final classification layers\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Less training data needed**: Pre-trained features work well even with small datasets\n",
    "- **Faster training**: Only train a small portion of the network\n",
    "- **Better performance**: Often achieves higher accuracy than training from scratch\n",
    "- **Lower computational cost**: No need for weeks of training on expensive GPUs\n",
    "\n",
    "### When Does Transfer Learning Work?\n",
    "\n",
    "Transfer learning is most effective when:\n",
    "\n",
    "- **Similar domains**: Source and target tasks both involve images\n",
    "- **Related features**: Low-level features (edges, textures) transfer well\n",
    "- **Limited target data**: Small datasets benefit most from pre-trained knowledge\n",
    "\n",
    "### Two Main Strategies\n",
    "\n",
    "1. **Feature extraction**: Freeze the pre-trained backbone, train only new layers\n",
    "2. **Fine-tuning**: Unfreeze some layers and train with a small learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Models in Keras\n",
    "\n",
    "Keras provides access to state-of-the-art CNN architectures pre-trained on ImageNet through `tf.keras.applications`.\n",
    "\n",
    "### Available Models\n",
    "\n",
    "Some popular options include:\n",
    "\n",
    "| Model          | Parameters | Speed  | Accuracy | Best For                |\n",
    "|----------------|-----------|--------|----------|------------------------|\n",
    "| MobileNetV2    | 3.5M      | Fast   | Good     | Mobile/embedded devices |\n",
    "| EfficientNetB0 | 5.3M      | Fast   | Better   | Balanced accuracy/speed |\n",
    "| ResNet50       | 25.6M     | Medium | High     | When accuracy matters   |\n",
    "| VGG16          | 138M      | Slow   | Good     | Simple architecture     |\n",
    "| InceptionV3    | 23.9M     | Medium | High     | Multi-scale features    |\n",
    "\n",
    "### MobileNetV2\n",
    "\n",
    "We'll use **MobileNetV2** as our backbone:\n",
    "\n",
    "- Designed for mobile and embedded devices\n",
    "- Excellent balance of accuracy and speed\n",
    "- Relatively few parameters (faster to fine-tune)\n",
    "- Pre-trained on ImageNet (1000 classes of everyday objects)\n",
    "\n",
    "### ImageNet Pre-training\n",
    "\n",
    "ImageNet is a large visual database with:\n",
    "\n",
    "- 1.4 million training images\n",
    "- 1000 classes (animals, plants, vehicles, objects)\n",
    "- Many natural images and outdoor scenes\n",
    "\n",
    "The learned features from ImageNet transfer well to many vision tasks, including flower classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction with Frozen Backbone\n",
    "\n",
    "The first transfer learning strategy is **feature extraction**: use the pre-trained model as a fixed feature extractor.\n",
    "\n",
    "### Loading the Pre-trained Model\n",
    "\n",
    "We load MobileNetV2 with ImageNet weights, excluding the top classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 without the classification head (include_top=False removes final Dense layer)\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=image_size + (3,),\n",
    "    include_top=False,  # Exclude final classification layer\n",
    "    weights='imagenet'  # Use ImageNet pre-trained weights\n",
    ")\n",
    "\n",
    "print(f'Base model layers: {len(base_model.layers)}')\n",
    "print(f'Base model output shape: {base_model.output_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Backbone\n",
    "\n",
    "We freeze the base model to prevent its weights from being updated during training. This preserves the learned ImageNet features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model - prevents weights from updating during training\n",
    "base_model.trainable = False\n",
    "\n",
    "print(f'Trainable weights before freezing: {len(base_model.trainable_weights)}')\n",
    "print(f'Non-trainable weights after freezing: {len(base_model.non_trainable_weights)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Model-Specific Preprocessing\n",
    "\n",
    "**Critical:** Each pre-trained model expects inputs preprocessed in a specific way. Using incorrect preprocessing will result in poor performance because the model was trained with specific input ranges and transformations.\n",
    "\n",
    "Different models use different preprocessing strategies:\n",
    "\n",
    "- **MobileNetV2**: Scales pixel values from [0, 255] to [-1, 1] range\n",
    "- **ResNet/VGG**: Subtracts ImageNet mean [103.939, 116.779, 123.68] from RGB channels\n",
    "- **EfficientNet**: Scales to [0, 1] then normalizes with ImageNet statistics\n",
    "- **InceptionV3**: Scales to [-1, 1] range\n",
    "\n",
    "**Always use the model's preprocessing function** (`tf.keras.applications.{model_name}.preprocess_input`) to ensure correct input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MobileNetV2-specific preprocessing function\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "# This function scales pixel values to [-1, 1] range (MobileNetV2's expected input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Transfer Learning Model\n",
    "\n",
    "We build a new model that:\n",
    "\n",
    "1. Takes images as input\n",
    "2. Preprocesses them for MobileNetV2\n",
    "3. Passes through the frozen backbone (feature extraction)\n",
    "4. Pools features to a single vector per channel\n",
    "5. Classifies using new dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the transfer learning model using Functional API\n",
    "inputs = keras.Input(shape=image_size + (3,))\n",
    "\n",
    "# Preprocessing: scale to [-1, 1] range expected by MobileNetV2\n",
    "x = preprocess_input(inputs)\n",
    "\n",
    "# Pre-trained backbone (frozen): extract high-level features\n",
    "# training=False keeps batch normalization layers in inference mode\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "# Classifier head: convert features to class predictions\n",
    "x = layers.GlobalAveragePooling2D()(x)  # More efficient than Flatten, reduces parameters\n",
    "x = layers.Dropout(0.5)(x)  # Regularization to prevent overfitting\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)  # Final classification\n",
    "\n",
    "model_transfer = keras.Model(inputs, outputs, name='transfer_learning_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer.summary()\n",
    "\n",
    "# Count trainable vs non-trainable parameters\n",
    "trainable_count = sum([tf.size(w).numpy() for w in model_transfer.trainable_weights])\n",
    "non_trainable_count = sum([tf.size(w).numpy() for w in model_transfer.non_trainable_weights])\n",
    "\n",
    "print(f'\\nTrainable parameters: {trainable_count:,}')\n",
    "print(f'Non-trainable parameters: {non_trainable_count:,}')\n",
    "print(f'Total parameters: {trainable_count + non_trainable_count:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Most parameters are frozen (non-trainable) - preserves ImageNet knowledge\n",
    "- Only the new classification layers are trainable - adapts to flower classes\n",
    "- Training will be fast since we update very few parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We compile and train with the frozen backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer.compile(\n",
    "    optimizer='adam',  # Default learning rate (1e-3) is fine for new layers\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "epochs_initial = 10\n",
    "history_transfer = model_transfer.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs_initial\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_training_history(history, title='Training History'):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Plot loss\n",
    "    ax1.plot(hist['epoch'], hist['loss'], label='Train Loss', marker='o')\n",
    "    ax1.plot(hist['epoch'], hist['val_loss'], label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy', marker='o')\n",
    "    ax2.plot(hist['epoch'], hist['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history_transfer, 'Transfer Learning (Frozen Backbone)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_transfer, val_acc_transfer = model_transfer.evaluate(val_ds, verbose=0)\n",
    "print(f'Transfer Learning Results (Frozen Backbone):')\n",
    "print(f'  Validation Loss: {val_loss_transfer:.4f}')\n",
    "print(f'  Validation Accuracy: {val_acc_transfer:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Let's visualize some predictions on validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on one batch\n",
    "for images, labels in val_ds.take(1):\n",
    "    predictions = model_transfer.predict(images, verbose=0)\n",
    "    predicted_classes = predictions.argmax(axis=-1)\n",
    "    \n",
    "    # Display some examples\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i in range(6):\n",
    "        ax = plt.subplot(2, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        true_class = class_names[labels[i]]\n",
    "        pred_class = class_names[predicted_classes[i]]\n",
    "        confidence = predictions[i][predicted_classes[i]]\n",
    "        \n",
    "        # Color code: green if correct, red if wrong\n",
    "        color = 'green' if true_class == pred_class else 'red'\n",
    "        plt.title(f'True: {true_class}\\nPred: {pred_class} ({confidence:.2f})', color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Custom CNN with Pre-trained Features\n",
    "\n",
    "A hybrid approach uses both custom convolutional layers and pre-trained features. This is useful when:\n",
    "\n",
    "- Your dataset has unique characteristics not well represented in ImageNet\n",
    "- You want to learn task-specific low-level features while leveraging pre-trained high-level features\n",
    "- You have sufficient data to train custom layers without overfitting\n",
    "\n",
    "### Building a Hybrid Model\n",
    "\n",
    "We create two parallel branches:\n",
    "\n",
    "1. **Custom CNN branch**: Learns task-specific features from scratch\n",
    "2. **Pre-trained branch**: Extracts general features from ImageNet\n",
    "\n",
    "Then concatenate and combine both feature sets for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hybrid model with two parallel branches\n",
    "inputs = keras.Input(shape=image_size + (3,))\n",
    "x = preprocess_input(inputs)  # Preprocess for MobileNetV2\n",
    "\n",
    "# Branch 1: Custom CNN path - learns flower-specific features\n",
    "custom_branch = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "custom_branch = layers.MaxPooling2D(2)(custom_branch)\n",
    "custom_branch = layers.Conv2D(64, 3, activation='relu', padding='same')(custom_branch)\n",
    "custom_branch = layers.GlobalAveragePooling2D()(custom_branch)  # Reduces to 64 features\n",
    "\n",
    "# Branch 2: Pre-trained features - extracts general visual features\n",
    "pretrained_branch = base_model(x, training=False)  # Keep frozen\n",
    "pretrained_branch = layers.GlobalAveragePooling2D()(pretrained_branch)  # Reduces to 1280 features\n",
    "\n",
    "# Combine both branches: concatenate custom (64) + pretrained (1280) = 1344 features\n",
    "combined = layers.Concatenate()([custom_branch, pretrained_branch])\n",
    "combined = layers.Dense(128, activation='relu')(combined)  # Learn feature combinations\n",
    "combined = layers.Dropout(0.5)(combined)  # Regularization\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(combined)\n",
    "\n",
    "model_hybrid = keras.Model(inputs, outputs, name='hybrid_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hybrid.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train hybrid model\n",
    "history_hybrid = model_hybrid.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "val_loss_hybrid, val_acc_hybrid = model_hybrid.evaluate(val_ds, verbose=0)\n",
    "print(f'\\nHybrid Model Results:')\n",
    "print(f'  Validation Loss: {val_loss_hybrid:.4f}')\n",
    "print(f'  Validation Accuracy: {val_acc_hybrid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid Model Characteristics:**\n",
    "\n",
    "- Trains both custom and pre-trained pathways\n",
    "- Custom layers can learn flower-specific patterns (petal textures, colors)\n",
    "- Pre-trained features provide robust general visual understanding\n",
    "- More trainable parameters than frozen transfer learning\n",
    "- Useful when your domain has unique characteristics not in ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Fine-tuning is the second transfer learning strategy. After training the new classification head with a frozen backbone, we:\n",
    "\n",
    "1. **Unfreeze** some or all of the backbone layers\n",
    "2. **Continue training** with a **much lower learning rate**\n",
    "3. Allow the pre-trained weights to adapt slightly to our specific task\n",
    "\n",
    "### Why Fine-tune?\n",
    "\n",
    "- **Better adaptation**: Backbone features adjust to our specific dataset\n",
    "- **Higher accuracy**: Often achieves better results than frozen features\n",
    "- **Task-specific features**: Top layers learn patterns specific to flowers\n",
    "\n",
    "### Risks and Precautions\n",
    "\n",
    "- **Catastrophic forgetting**: High learning rate can destroy pre-trained knowledge\n",
    "- **Overfitting**: More trainable parameters increase overfitting risk\n",
    "- **Solution**: Use very small learning rate (10-100\u00d7 smaller than initial training)\n",
    "\n",
    "### Unfreezing the Backbone\n",
    "\n",
    "We can unfreeze:\n",
    "- **All layers**: Maximum flexibility\n",
    "- **Top N layers**: Keeps early generic features frozen\n",
    "\n",
    "Let's unfreeze the entire backbone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model - allows weights to update during training\n",
    "base_model.trainable = True\n",
    "\n",
    "print(f'Number of layers in base model: {len(base_model.layers)}')\n",
    "print(f'Trainable layers: {sum([1 for layer in base_model.layers if layer.trainable])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompiling with Lower Learning Rate\n",
    "\n",
    "**Critical:** Use a much smaller learning rate for fine-tuning to avoid destroying pre-trained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with very low learning rate (100\u00d7 smaller than default)\n",
    "# This prevents catastrophic forgetting of ImageNet features\n",
    "model_transfer.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),  # Default is 1e-3\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model recompiled with learning rate: 1e-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning Training\n",
    "\n",
    "Continue training for a few more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_finetune = 5\n",
    "total_epochs = epochs_initial + epochs_finetune\n",
    "\n",
    "# Continue training from where we left off\n",
    "history_finetune = model_transfer.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=history_transfer.epoch[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Fine-tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history_finetune, 'Fine-tuning')\n",
    "\n",
    "val_loss_finetune, val_acc_finetune = model_transfer.evaluate(val_ds, verbose=0)\n",
    "print(f'\\nFine-tuning Results:')\n",
    "print(f'  Validation Loss: {val_loss_finetune:.4f}')\n",
    "print(f'  Validation Accuracy: {val_acc_finetune:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Training Curve\n",
    "\n",
    "Let's visualize the complete training process: frozen backbone + fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories from both training phases\n",
    "acc = history_transfer.history['accuracy'] + history_finetune.history['accuracy']\n",
    "val_acc = history_transfer.history['val_accuracy'] + history_finetune.history['val_accuracy']\n",
    "loss = history_transfer.history['loss'] + history_finetune.history['loss']\n",
    "val_loss = history_transfer.history['val_loss'] + history_finetune.history['val_loss']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(loss, label='Train Loss', marker='o')\n",
    "ax1.plot(val_loss, label='Val Loss', marker='s')\n",
    "ax1.axvline(x=epochs_initial-1, color='r', linestyle='--', label='Start Fine-tuning')  # Mark transition\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Complete Training: Frozen + Fine-tuning')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(acc, label='Train Accuracy', marker='o')\n",
    "ax2.plot(val_acc, label='Val Accuracy', marker='s')\n",
    "ax2.axvline(x=epochs_initial-1, color='r', linestyle='--', label='Start Fine-tuning')  # Mark transition\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Complete Training: Frozen + Fine-tuning')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Approaches\n",
    "\n",
    "Let's summarize the different approaches we could use:\n",
    "\n",
    "### Training from Scratch (Previous Notebook)\n",
    "\n",
    "- Train all weights randomly initialized\n",
    "- Requires more data and epochs\n",
    "- Higher computational cost\n",
    "\n",
    "### Transfer Learning (Frozen)\n",
    "\n",
    "- Freeze pre-trained backbone\n",
    "- Train only new classification layers\n",
    "- Fast training, good baseline results\n",
    "\n",
    "### Hybrid Model\n",
    "\n",
    "- Combines custom CNN with pre-trained features\n",
    "- Learns task-specific features while leveraging ImageNet knowledge\n",
    "- More parameters than frozen, less than full fine-tuning\n",
    "\n",
    "### Fine-tuning\n",
    "\n",
    "- Start with frozen backbone training\n",
    "- Then unfreeze and train with low learning rate\n",
    "- Best accuracy, moderate training time\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Approach                  | Trainable Params | Training Time | Typical Accuracy | When to Use                          |\n",
    "|---------------------------|------------------|---------------|------------------|--------------------------------------|\n",
    "| From scratch (Notebook 05)| All (~500K-2M)   | Slow          | Baseline         | Large dataset, unique domain         |\n",
    "| Frozen backbone           | Few (~10K-50K)   | Fast          | Good             | Limited data, similar to ImageNet    |\n",
    "| Hybrid model              | Medium (~50K-200K)| Medium        | Good-Better      | Dataset with unique characteristics  |\n",
    "| Fine-tuning               | All (~500K-3M)   | Medium        | Best             | Want highest accuracy, have some data|\n",
    "\n",
    "### Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Transfer Learning (Frozen):  {val_acc_transfer:.4f}\")\n",
    "print(f\"Hybrid Model:                {val_acc_hybrid:.4f}\")\n",
    "print(f\"Fine-tuning:                 {val_acc_finetune:.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nImprovement from fine-tuning: {(val_acc_finetune - val_acc_transfer):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Typical observations:**\n",
    "\n",
    "- Transfer learning achieves good results quickly (70-80% accuracy in 10 epochs)\n",
    "- Hybrid models can match or exceed frozen transfer learning\n",
    "- Fine-tuning usually improves accuracy by 2-5%\n",
    "- All approaches vastly outperform training from scratch with limited data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "**Transfer Learning Concept:**\n",
    "- Reuse knowledge from models trained on large datasets (ImageNet with 1.4M images)\n",
    "- Reduces data requirements and training time significantly\n",
    "- Often achieves better results than training from scratch, especially with limited data\n",
    "\n",
    "**Pre-trained Models:**\n",
    "- Keras Applications provides many CNN architectures (MobileNetV2, EfficientNet, ResNet)\n",
    "- Models pre-trained on ImageNet (1000 classes: animals, plants, objects)\n",
    "- Different models offer tradeoffs: MobileNetV2 (fast), ResNet50 (accurate), EfficientNet (balanced)\n",
    "\n",
    "**Model-Specific Preprocessing:**\n",
    "- Each pre-trained model expects specific input format\n",
    "- Always use model's preprocessing function (e.g., `mobilenet_v2.preprocess_input`)\n",
    "- MobileNetV2 scales to [-1, 1], ResNet subtracts mean, EfficientNet normalizes\n",
    "\n",
    "**Feature Extraction:**\n",
    "- Freeze pre-trained backbone (`trainable=False`)\n",
    "- Train only new classification layers\n",
    "- Fast training, provides good baseline performance\n",
    "\n",
    "**Hybrid Approach:**\n",
    "- Combine custom CNN branch with pre-trained features\n",
    "- Learn task-specific patterns while leveraging ImageNet knowledge\n",
    "- Useful when dataset has unique characteristics\n",
    "\n",
    "**Fine-tuning:**\n",
    "- Unfreeze backbone after initial training with frozen weights\n",
    "- Continue training with very low learning rate (e.g., 1e-5 vs default 1e-3)\n",
    "- Allows weights to adapt to specific task without forgetting ImageNet features\n",
    "- Usually improves accuracy by 2-5% over frozen features\n",
    "\n",
    "**Best Practices:**\n",
    "- Always start with frozen backbone training\n",
    "- Use correct preprocessing for chosen backbone\n",
    "- Use very low learning rate for fine-tuning (100\u00d7 smaller)\n",
    "- Monitor validation metrics to avoid overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}