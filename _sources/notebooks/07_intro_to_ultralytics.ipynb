{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ultralytics YOLO\n",
    "\n",
    "Ultralytics provides state-of-the-art object detection models through the YOLO (You Only Look Once) family. YOLO models are known for their speed and accuracy, making them suitable for real-time object detection tasks in applications like autonomous driving, surveillance, retail, and robotics.\n",
    "\n",
    "In this notebook we take a high-level look at the Ultralytics ecosystem and run our first detections with modern YOLO models. We use YOLO11 as our primary example, but the concepts apply to the broader YOLO family.\n",
    "\n",
    "> **Note:** Ultralytics YOLO is a family of models (YOLOv3\u2013YOLO11 and beyond). In this course we use YOLO11 models (e.g., `yolo11n.pt`) as a concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#Setup-and-Imports)\n",
    "2. [Example Images](#Example-Images)\n",
    "3. [What is YOLO?](#What-is-YOLO?)\n",
    "4. [Evolution of the YOLO Family](#Evolution-of-the-YOLO-Family)\n",
    "5. [Ultralytics Framework Overview](#Ultralytics-Framework-Overview)\n",
    "6. [Loading a YOLO11 Detection Model](#Loading-a-YOLO11-Detection-Model)\n",
    "7. [First Detection on a Single Image](#First-Detection-on-a-Single-Image)\n",
    "8. [Inspecting the Results Object](#Inspecting-the-Results-Object)\n",
    "9. [Other YOLO Tasks](#Other-YOLO-Tasks)\n",
    "10. [Recap and Exercises](#Recap-and-Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's set up the environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install Ultralytics and OpenCV in fresh environments (e.g. Colab)\n",
    "# %pip install ultralytics opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Example Images\n",
    "\n",
    "This section downloads the example images used throughout the Ultralytics notebooks. Images are saved locally in `../images/` so they only need to be downloaded once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "def download_images(image_urls, target_dir=\"../images\"):\n",
    "    \"\"\"\n",
    "    Download images from URLs to a local directory.\n",
    "    \n",
    "    Args:\n",
    "        image_urls: List of (url, filename) tuples\n",
    "        target_dir: Target directory for saving images\n",
    "    \n",
    "    Returns:\n",
    "        List of local file paths\n",
    "    \"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up opener with User-Agent to avoid 403 errors\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')]\n",
    "    urllib.request.install_opener(opener)\n",
    "    \n",
    "    # Handle SSL certificate issues\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    \n",
    "    downloaded_paths = []\n",
    "    \n",
    "    for url, filename in image_urls:\n",
    "        file_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"\u2713 {filename} already exists ({size_mb:.2f} MB)\")\n",
    "        else:\n",
    "            try:\n",
    "                print(f\"Downloading {filename}...\", end=\" \")\n",
    "                urllib.request.urlretrieve(url, file_path)\n",
    "                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                print(f\"\u2713 ({size_mb:.2f} MB)\")\n",
    "            except Exception as e:\n",
    "                print(f\"\u2717 Failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        downloaded_paths.append(file_path)\n",
    "    \n",
    "    return downloaded_paths\n",
    "\n",
    "# Define all example images for the Ultralytics notebooks\n",
    "image_urls = [\n",
    "    # Object detection examples\n",
    "    ('https://akm-img-a-in.tosshub.com/indiatoday/images/story/201812/dogs_and_cats.jpeg?TAxD19DTCFE7WiSYLUdTu446cfW4AbuW&size=770:433', 'yolo_dog_cat.jpg'),\n",
    "    ('https://i.ibb.co/R7pRTLy/beach-no-axis.png', 'yolo_beach_scene.jpg'),\n",
    "    ('https://i.ibb.co/jL1kZRF/phones.png', 'yolo_phones_on_table.jpg'),\n",
    "    ('https://i.ytimg.com/vi/1ZupwFOhjl4/maxresdefault.jpg', 'yolo_traffic.jpg'),\n",
    "    # Segmentation example\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/d/d3/Albert_Einstein_Head.jpg', 'yolo_einstein_head.jpg'),\n",
    "    # Pose estimation examples\n",
    "    ('https://images.unsplash.com/photo-1561049501-e1f96bdd98fd?q=80&w=2778&auto=format&fit=crop&ixlib=rb-4.0.3', 'yolo_yoga_1.jpg'),\n",
    "    ('https://images.unsplash.com/photo-1545205597-3d9d02c29597?q=80&w=2940&auto=format&fit=crop&ixlib=rb-4.0.3', 'yolo_yoga_2.jpg'),\n",
    "]\n",
    "\n",
    "# Download all images\n",
    "downloaded_paths = download_images(image_urls)\n",
    "print(f\"\\nTotal images available: {len(downloaded_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Images\n",
    "\n",
    "Let's verify that all images are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all downloaded YOLO images\n",
    "image_dir = \"../images\"\n",
    "yolo_images = sorted([f for f in os.listdir(image_dir) if f.startswith(\"yolo_\")])\n",
    "\n",
    "print(\"Available YOLO example images:\")\n",
    "print(\"-\" * 45)\n",
    "for img in yolo_images:\n",
    "    img_path = os.path.join(image_dir, img)\n",
    "    size_mb = os.path.getsize(img_path) / (1024 * 1024)\n",
    "    print(f\"  {img:<30} {size_mb:>6.2f} MB\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"Total: {len(yolo_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a few sample images\n",
    "sample_images = [\"yolo_beach_scene.jpg\", \"yolo_traffic.jpg\", \"yolo_dog_cat.jpg\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, img_name in zip(axes, sample_images):\n",
    "    img_path = os.path.join(image_dir, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(img_rgb)\n",
    "    ax.set_title(img_name.replace(\"yolo_\", \"\").replace(\".jpg\", \"\"))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Sample Images for Object Detection\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of the YOLO Family\n",
    "\n",
    "YOLO has evolved significantly since its introduction:\n",
    "\n",
    "- **YOLOv1\u2013YOLOv3** (2016\u20132018): Established real-time detection. YOLOv3 introduced multi-scale prediction.\n",
    "- **YOLOv4** (2020): Improved training techniques (Bag of Specials/Freebies), better small object detection.\n",
    "- **YOLOv5** (2020): First Ultralytics version, PyTorch-based, easy training and deployment.\n",
    "- **YOLOv6\u2013YOLOv7** (2022): Various improvements in architecture and efficiency.\n",
    "- **YOLOv8** (2023): Ultralytics flagship, anchor-free detection, improved accuracy.\n",
    "- **YOLOv9\u2013YOLOv10** (2024): Further architectural improvements.\n",
    "- **YOLO11** (2024): Current generation we use in this course.\n",
    "\n",
    "Each version brings improvements in speed, accuracy, or ease of use. **Ultralytics wraps these model families into a unified, user-friendly Python API** that you will use throughout the rest of this course. This means you can switch between model versions or tasks (detection, segmentation, pose) with minimal code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultralytics Framework Overview\n",
    "\n",
    "The Ultralytics framework provides a consistent API for working with YOLO models:\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"model_file.pt\")\n",
    "results = model(source)\n",
    "```\n",
    "\n",
    "### Supported Tasks\n",
    "\n",
    "The same API works for different computer vision tasks:\n",
    "\n",
    "- **Object Detection**: Locate and classify objects with bounding boxes\n",
    "- **Instance Segmentation**: Detect objects and their precise pixel masks\n",
    "- **Image Classification**: Classify entire images into categories\n",
    "- **Pose Estimation**: Detect human figures and key body points\n",
    "- **Oriented Bounding Boxes (OBB)**: Rotated boxes for angled objects\n",
    "- **Object Tracking**: Track objects across video frames\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Pre-trained models ready to use\n",
    "- Simple training and fine-tuning\n",
    "- Export to various formats (ONNX, TensorRT, CoreML, etc.)\n",
    "- Extensive documentation and community support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a YOLO11 Detection Model\n",
    "\n",
    "Let's load a YOLO11 detection model. Ultralytics provides different model sizes optimized for various use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO11 nano model - fast and lightweight\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "print(f\"Model loaded: {model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO11 Model Variants\n",
    "\n",
    "YOLO11 comes in different sizes, each balancing speed and accuracy:\n",
    "\n",
    "| Model | Size | Speed | Accuracy | Best For |\n",
    "|-------|------|-------|----------|----------|\n",
    "| `yolo11n.pt` | Nano | Fastest | Good | Edge devices, mobile, real-time apps |\n",
    "| `yolo11s.pt` | Small | Fast | Better | Balanced performance |\n",
    "| `yolo11m.pt` | Medium | Moderate | High | General-purpose detection |\n",
    "| `yolo11l.pt` | Large | Slower | Higher | Accuracy-focused applications |\n",
    "| `yolo11x.pt` | Extra Large | Slowest | Highest | Maximum accuracy, offline processing |\n",
    "\n",
    "The trade-off is simple:\n",
    "- **Smaller models** \u2192 faster inference, lower accuracy, less memory\n",
    "- **Larger models** \u2192 slower inference, higher accuracy, more memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Detection on a Single Image\n",
    "\n",
    "Let's run our first object detection on a traffic scene image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image using local path\n",
    "image_path = \"../images/yolo_traffic.jpg\"\n",
    "\n",
    "img_bgr = cv2.imread(image_path)\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(\"Original image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection - pass the RGB image to the model\n",
    "results = model(img_rgb)\n",
    "\n",
    "# Get annotated image with bounding boxes\n",
    "annotated = results[0].plot()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(annotated)\n",
    "plt.title(\"YOLO11 detections\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we see:**\n",
    "\n",
    "- Bounding boxes around detected objects (cars, trucks, buses, traffic lights, etc.)\n",
    "- Class labels showing what each object is\n",
    "- Confidence scores indicating model certainty\n",
    "- Multiple objects detected in a single pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Results Object\n",
    "\n",
    "The `results` object contains detailed information about the detections. Let's explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first (and only) result\n",
    "r = results[0]\n",
    "\n",
    "# Show available class names (COCO dataset classes)\n",
    "print(\"Available classes (first 10):\")\n",
    "for idx, name in list(r.names.items())[:10]:\n",
    "    print(f\"  {idx} \u2192 {name}\")\n",
    "\n",
    "print(f\"\\nTotal COCO classes: {len(r.names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all detected objects with their confidence scores\n",
    "print(\"Detected objects:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, box in enumerate(r.boxes):\n",
    "    cls_id = int(box.cls)  # Class ID (integer)\n",
    "    conf = float(box.conf)  # Confidence score (0-1)\n",
    "    class_name = r.names[cls_id]  # Human-readable class name\n",
    "    print(f\"{i+1}. {class_name:15} (confidence: {conf:.2f})\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total detections: {len(r.boxes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect bounding box coordinates\n",
    "print(\"First bounding box details:\")\n",
    "print(f\"  Format: xyxy (x1, y1, x2, y2)\")\n",
    "print(f\"  Coordinates: {r.boxes.xyxy[0].cpu().numpy()}\")\n",
    "print(f\"\\nCoordinate meaning:\")\n",
    "print(f\"  x1, y1 = top-left corner\")\n",
    "print(f\"  x2, y2 = bottom-right corner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other YOLO Tasks\n",
    "\n",
    "Beyond object detection, Ultralytics YOLO supports several other computer vision tasks. The model file suffix indicates the task type.\n",
    "\n",
    "### Task-Specific Models\n",
    "\n",
    "| Task | Model File | Description |\n",
    "|------|-----------|-------------|\n",
    "| Detection | `yolo11n.pt` | Bounding boxes around objects |\n",
    "| Segmentation | `yolo11n-seg.pt` | Pixel-level object masks |\n",
    "| Classification | `yolo11n-cls.pt` | Image-level classification |\n",
    "| Pose | `yolo11n-pose.pt` | Human keypoint detection |\n",
    "| OBB | `yolo11n-obb.pt` | Oriented (rotated) bounding boxes |\n",
    "\n",
    "Let's try a quick segmentation example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap and Exercises\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **YOLO** (You Only Look Once) is a real-time object detection system that processes entire images in a single pass\n",
    "- **Ultralytics** provides a unified, easy-to-use API for YOLO models\n",
    "- **Loading a model** is simple: `model = YOLO(\"yolo11n.pt\")`\n",
    "- **Running detection**: `results = model(image)` returns detailed results\n",
    "- **Inspecting results**: Access bounding boxes (`.boxes.xyxy`), confidence (`.boxes.conf`), and class names (`.names`)\n",
    "- **Multiple tasks**: Same API supports detection, segmentation, pose estimation, and more\n",
    "- **Model variants**: Choose between nano (fast) to xlarge (accurate) based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exercise 1: Compare different model sizes\n\nimage_path = \"../images/yolo_traffic.jpg\"\nimg = cv2.imread(image_path)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nmodel_names = [\"yolo11n.pt\", \"yolo11s.pt\"]\n\nfor model_name in model_names:\n    # TODO: Load the model\n    # Hint: Use YOLO(model_name)\n    \n    # TODO: Measure inference time\n    # Hint: Use time.time() before and after model inference\n    # Hint: Convert to milliseconds by multiplying by 1000\n    \n    # TODO: Count detections\n    # Hint: len(results[0].boxes) gives the count\n    \n    # TODO: Print results\n    # Hint: Print model name, detection count, and inference time\n    \n    pass  # Remove this line when you complete the TODO",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Exercise 2: Count Unique Classes\n\nChoose another image and count how many different object classes appear in the predictions."
  },
  {
   "cell_type": "code",
   "source": "# Exercise 2: Count unique classes in detections\n\nimage_path = \"../images/yolo_beach_scene.jpg\"\n\n# TODO: Load and run detection\n# Hint: Use cv2.imread(), cv2.cvtColor(), YOLO(), and model(img_rgb)\n\n# TODO: Get unique class names\n# Hint: Create a set to store unique names\n# Hint: Loop over results[0].boxes\n# Hint: Use int(box.cls) to get class ID, then r.names[cls_id] for name\n\n# TODO: Print results\n# Hint: Print the count and list of unique class names\n\npass  # Remove this line when you complete the TODO",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}